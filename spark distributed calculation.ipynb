{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Spark Distributed Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Find the bad XML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\",\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datacourse/spark/miniprojects\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000.xml.gz  part-00003.xml.gz  part-00006.xml.gz  part-00009.xml.gz\r\n",
      "part-00001.xml.gz  part-00004.xml.gz  part-00007.xml.gz  part-00010.xml.gz\r\n",
      "part-00002.xml.gz  part-00005.xml.gz  part-00008.xml.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls spark-stats-data/allPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = sc.textFile(localpath('spark-stats-data/allPosts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n",
       " '<parent>',\n",
       " '  <row Body=\"\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73933\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"5\" Score=\"0\" />',\n",
       " '  ',\n",
       " '  <row Body=\"See `continuous-data`\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73934\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"4\" Score=\"0\" />']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xmls=raw.filter(lambda x: '<row' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109522"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=etree.fromstring('''<row Body=\"\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73933\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"5\" Score=\"0\" />''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.getchildren()\n",
    "'Score' in a.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XMLsyntaxError\n",
    "import xml.etree.ElementTree as ET      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePost(line):\n",
    "    try:\n",
    "        root = ET.fromstring(line)\n",
    "        return line\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = xmls.map(parsePost).filter(lambda x: x!= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108741"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  <row Body=\"\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73933\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"5\" Score=\"0\" />',\n",
       " '  <row Body=\"See `continuous-data`\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73934\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"4\" Score=\"0\" />',\n",
       " '  <row Body=\"&lt;p&gt;O.K., I think I found a way of doing it with the correct assumptions. Although it is only useful for my particular problem, maybe somebody can tell me if I am being too &quot;sloppy&quot;, correct me or maybe my approach might be useful to somebody in the future.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, I had not realized that the &quot;triangles&quot; account for &quot;long  independent events&quot;. This means that $P(X_i = 0 | \\\\sum_{j\\\\neq i, |j-i| &amp;lt; D} X_j = 1) = 1$ in my notation ($X_i$ represents that an event starts in the moment $i$). What I started doing was a smoothing of $2D$ of $p_i$, so I took the average (can I do this?) in windows. This gave me a way of seeing how many events are there in a sequence.&#10;&lt;img src=&quot;http://i.imgur.com/YGlFKP0.png&quot; alt=&quot;The sequence pi&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The orange line is the original $p_i$ sequence, the black one is the smoothed. After this, in a window of length $2K$, I count how many peaks are there (how many possible events), and the probability of each event is the sum of probabilities from the beginning until the end of the &quot;hill&quot;, although, as can be seen in the picture, sometimes they can overlap, but I have no idea of how can I take that into account. Then, the probability of no event happening in the window of length $2K$ is the product of probabilities of no &quot;long event&quot; happening in this window.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you think it is a good answer? Do you have any comment/suggestion? Thank you very much.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:49:42.880\" Id=\"73936\" LastActivityDate=\"2013-10-28T10:49:42.880\" OwnerUserId=\"28322\" ParentId=\"73931\" PostTypeId=\"2\" Score=\"0\" />',\n",
       " '  <row Body=\"&lt;p&gt;First, we\\'ll need to know whether you are interested in the response to each Likert question or to a sum of Likert questions; if the latter, it matters how many questions and what the distribution of the scale looks like.&lt;/p&gt;&#10;&#10;&lt;p&gt;Either way, you will have to account for the nonindependence of the data, because the same people are answering the questions multiple times. Repeated measures ANOVA is one solution to this, but it makes unrealistic assumptions including sphericity, and would only be usable for the scale score, and only if the scores ranged fairly widely so that you could pretend they were continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;A better option is a mixed model. If you treat the scores as continuous data, then this would be a linear mixed model; if you treat them as ordinal (as you would have to do if you were interested in each question) then you would need a nonlinear mixed model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, these models are not simple to implement. If you currently know only about t-tests, then you may need to hire a consultant to help. &lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:53:25.943\" Id=\"73937\" LastActivityDate=\"2013-10-28T10:53:25.943\" OwnerUserId=\"686\" ParentId=\"65970\" PostTypeId=\"2\" Score=\"1\" />',\n",
       " '  <row AnswerCount=\"1\" Body=\"&lt;p&gt;I have two variables which represent two performance measures.&lt;br&gt;&#10;I ranked a finite set of elements according these two variables.&lt;br&gt;&#10;Therefore, I have to ranks. Suppose the ranks are performed in descending order (the highest the measure the highest the value in the decision process of each element).&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, an ordered set $\\\\Omega$ of $10$ elements according the two measures A and B.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R#A = [5 3 1 9 2 10 6 7 4 8];  &#10;R#B = [9 7 4 8 5 6 1 3 2 10];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Suppose that now I truncate R#A and R#B in order to select the &quot;top 5&quot; elements:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R#A_5 = [5 3 1 9 2];&#10;R#B_5 = [9 7 4 8 5];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In your opinion it is still possible to get the Spearman\\'s rho correlation coefficient with these two partial orders?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that&lt;br&gt;&#10;1) We are in the second step of the Spearman\\'s because we are alrady dealing with ranks.&lt;br&gt;&#10;2) The Sample size is very low but it is just for explanation. &lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T11:21:57.617\" Id=\"73938\" LastActivityDate=\"2014-11-24T21:25:49.137\" LastEditDate=\"2013-10-28T23:32:10.697\" LastEditorUserId=\"805\" OwnerUserId=\"9047\" PostTypeId=\"1\" Score=\"0\" Tags=\"&lt;ranking&gt;&lt;rank-correlation&gt;&lt;ranks&gt;&lt;spearman-rho&gt;\" Title=\"Spearman\\'s Rho - from partial ranked variables\" ViewCount=\"88\" />']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "bad_xml = 781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Favorites and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We're interested in looking for useful patterns in the data.  If we look at the Post data again (the smaller set, `stats.stackexchange.com`), we see that many things about each post are recorded.  We're going to start by looking to see if there is a relationship between the number of times a post was favorited (the `FavoriteCount`) and the `Score`.  The score is the number of times the post was upvoted minus the number of times it was downvoted, so it is a measure of how much a post was liked.  We'd expect posts with a higher number of favorites to have better scores, since they're both measurements of how good the post is.\n",
    "\n",
    "Let's aggregate posts by the number of favorites, and find the average score for each number of favorites.  Do this for the lowest 50 numbers of favorites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fav(line):\n",
    "    try:\n",
    "        root = ET.fromstring(line)\n",
    "        if \"Score\" and \"FavoriteCount\" in root.attrib:\n",
    "            score = root.attrib['Score']\n",
    "            fav = root.attrib['FavoriteCount']\n",
    "            return (int(fav),int(score))\n",
    "        else:\n",
    "            return ('Empty')\n",
    "    except:\n",
    "        return ('Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "favs=posts.map(fav).filter(lambda x: x != 'Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2051), (11, 1034), (22, 422), (33, 221), (44, 76)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favs.sortByKey()\\\n",
    "    .reduceByKey(lambda x,y: x+y).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favs.sortByKey(lambda x: x[0])\\\n",
    "    .reduceByKey(lambda x,y: x+y).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1045), (11, 59), (22, 13), (33, 6), (44, 1)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favs.map(lambda x: x[0]).map(lambda x: (x,1))\\\n",
    "    .sortByKey(lambda x: x[0])\\\n",
    "    .reduceByKey(lambda x,y: x+y).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favs.map(lambda x: x[0]).map(lambda x: (x,1))\\\n",
    "    .reduceByKey(lambda x,y: x+y).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = favs.sortByKey(lambda x: x[0])\\\n",
    "            .reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "count = favs.map(lambda x: x[0]).map(lambda x: (x,1))\\\n",
    "            .sortByKey(lambda x: x[0])\\\n",
    "            .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.9626794258373206),\n",
       " (11, 17.52542372881356),\n",
       " (22, 32.46153846153846),\n",
       " (33, 36.833333333333336),\n",
       " (44, 76.0),\n",
       " (55, 68.0),\n",
       " (66, 66.0),\n",
       " (88, 75.0),\n",
       " (275, 222.0),\n",
       " (1, 2.7334613999279624),\n",
       " (12, 18.793650793650794),\n",
       " (23, 31.76923076923077),\n",
       " (34, 41.0),\n",
       " (45, 64.0),\n",
       " (67, 69.0),\n",
       " (100, 118.0),\n",
       " (155, 166.0),\n",
       " (2, 4.481914893617021),\n",
       " (13, 20.083333333333332),\n",
       " (24, 29.142857142857142),\n",
       " (35, 41.0),\n",
       " (79, 102.5),\n",
       " (3, 6.350249584026622),\n",
       " (14, 23.58823529411765),\n",
       " (25, 38.55555555555556),\n",
       " (36, 53.25),\n",
       " (47, 90.5),\n",
       " (58, 45.5),\n",
       " (69, 93.0),\n",
       " (91, 97.0),\n",
       " (102, 110.0),\n",
       " (113, 121.0),\n",
       " (4, 7.656934306569343),\n",
       " (15, 22.594594594594593),\n",
       " (26, 38.25),\n",
       " (37, 54.5),\n",
       " (48, 51.5),\n",
       " (59, 69.0),\n",
       " (70, 72.0),\n",
       " (103, 75.0),\n",
       " (158, 170.0),\n",
       " (5, 8.941888619854721),\n",
       " (16, 25.48148148148148),\n",
       " (27, 39.55555555555556),\n",
       " (38, 63.6),\n",
       " (49, 49.333333333333336),\n",
       " (60, 63.0),\n",
       " (6, 11.263779527559056),\n",
       " (17, 26.333333333333332),\n",
       " (28, 34.166666666666664),\n",
       " (39, 40.0),\n",
       " (50, 53.0),\n",
       " (61, 57.0),\n",
       " (72, 85.0),\n",
       " (83, 101.0),\n",
       " (138, 134.0),\n",
       " (7, 12.916666666666666),\n",
       " (18, 25.814814814814813),\n",
       " (29, 45.75),\n",
       " (40, 39.666666666666664),\n",
       " (73, 69.5),\n",
       " (95, 90.0),\n",
       " (238, 214.0),\n",
       " (8, 13.345864661654135),\n",
       " (19, 25.944444444444443),\n",
       " (30, 43.4),\n",
       " (41, 51.0),\n",
       " (52, 57.0),\n",
       " (96, 54.0),\n",
       " (118, 70.0),\n",
       " (129, 132.0),\n",
       " (9, 15.754237288135593),\n",
       " (20, 29.636363636363637),\n",
       " (31, 40.875),\n",
       " (42, 52.0),\n",
       " (64, 81.0),\n",
       " (141, 112.0),\n",
       " (10, 17.0),\n",
       " (21, 35.333333333333336),\n",
       " (32, 33.0),\n",
       " (54, 73.5),\n",
       " (65, 68.0),\n",
       " (76, 54.0)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.join(count).map(lambda x: (x[0],x[1][0]/x[1][1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#favorite_score = [(0, 2.3398827696988396)]*50\n",
    "favorite = list(total.join(count).map(lambda x: (x[0],x[1][0]/x[1][1])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.9626794258373206),\n",
       " (1, 2.7334613999279624),\n",
       " (2, 4.481914893617021),\n",
       " (3, 6.350249584026622),\n",
       " (4, 7.656934306569343),\n",
       " (5, 8.941888619854721),\n",
       " (6, 11.263779527559056),\n",
       " (7, 12.916666666666666),\n",
       " (8, 13.345864661654135),\n",
       " (9, 15.754237288135593),\n",
       " (10, 17.0),\n",
       " (11, 17.52542372881356),\n",
       " (12, 18.793650793650794),\n",
       " (13, 20.083333333333332),\n",
       " (14, 23.58823529411765),\n",
       " (15, 22.594594594594593),\n",
       " (16, 25.48148148148148),\n",
       " (17, 26.333333333333332),\n",
       " (18, 25.814814814814813),\n",
       " (19, 25.944444444444443),\n",
       " (20, 29.636363636363637),\n",
       " (21, 35.333333333333336),\n",
       " (22, 32.46153846153846),\n",
       " (23, 31.76923076923077),\n",
       " (24, 29.142857142857142),\n",
       " (25, 38.55555555555556),\n",
       " (26, 38.25),\n",
       " (27, 39.55555555555556),\n",
       " (28, 34.166666666666664),\n",
       " (29, 45.75),\n",
       " (30, 43.4),\n",
       " (31, 40.875),\n",
       " (32, 33.0),\n",
       " (33, 36.833333333333336),\n",
       " (34, 41.0),\n",
       " (35, 41.0),\n",
       " (36, 53.25),\n",
       " (37, 54.5),\n",
       " (38, 63.6),\n",
       " (39, 40.0),\n",
       " (40, 39.666666666666664),\n",
       " (41, 51.0),\n",
       " (42, 52.0),\n",
       " (44, 76.0),\n",
       " (45, 64.0),\n",
       " (47, 90.5),\n",
       " (48, 51.5),\n",
       " (49, 49.333333333333336),\n",
       " (50, 53.0),\n",
       " (52, 57.0)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favorite_score = sorted(favorite, key=lambda x: x[0])[:50]\n",
    "favorite_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Answer percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Investigate the correlation between a user's reputation and the kind of posts they make. For the 99 users with the highest reputation, single out posts which are either questions or answers and look at the percentage of these posts that are answers: *(answers / (answers + questions))*. \n",
    "\n",
    "Return a tuple of their **user ID** and this fraction.\n",
    "\n",
    "You should also return (-1, fraction) to represent the case where you average over all users (so you will return 100 entries total).\n",
    "\n",
    "Again, you only need to run this on the statistics overflow set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = sc.textFile(localpath('spark-stats-data/allUsers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n",
       " '<parent>',\n",
       " '  <row AboutMe=\"&lt;p&gt;Hi, I\\'m not really a person.&lt;/p&gt;&#10;&#10;&lt;p&gt;I\\'m a background process that helps keep this site clean!&lt;/p&gt;&#10;&#10;&lt;p&gt;I do things like&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Randomly poke old unanswered questions every hour so they get some attention&lt;/li&gt;&#10;&lt;li&gt;Own community questions and answers so nobody gets unnecessary reputation from them&lt;/li&gt;&#10;&lt;li&gt;Own downvotes on spam/evil posts that get permanently deleted&lt;/li&gt;&#10;&lt;li&gt;Own suggested edits from anonymous users&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://meta.stackexchange.com/a/92006&quot;&gt;Remove abandoned questions&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;\" AccountId=\"-1\" CreationDate=\"2010-07-19T06:55:26.860\" DisplayName=\"Community\" DownVotes=\"2330\" Id=\"-1\" LastAccessDate=\"2010-07-19T06:55:26.860\" Location=\"on the server farm\" Reputation=\"1\" UpVotes=\"5831\" Views=\"0\" WebsiteUrl=\"http://meta.stackexchange.com/\" />',\n",
       " '  ',\n",
       " '  <row AboutMe=\"&lt;p&gt;Developer on the StackOverflow team.  Find me on&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.twitter.com/SuperDalgas&quot; rel=&quot;nofollow&quot;&gt;Twitter&lt;/a&gt;&#10;&lt;br&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/&quot;&gt;Stack Overflow Valued Associate #00003&lt;/a&gt;&lt;/p&gt;&#10;\" AccountId=\"2\" Age=\"38\" CreationDate=\"2010-07-19T14:01:36.697\" DisplayName=\"Geoff Dalgas\" DownVotes=\"0\" Id=\"2\" LastAccessDate=\"2015-02-05T19:58:19.133\" Location=\"Corvallis, OR\" Reputation=\"101\" UpVotes=\"3\" Views=\"26\" WebsiteUrl=\"http://stackoverflow.com\" />']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##get reputation score\n",
    "def rep(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"Id\" and \"Reputation\" in root.attrib:\n",
    "                uid = root.attrib['Id']\n",
    "                rep = root.attrib['Reputation']\n",
    "                return (uid,int(rep))\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')\n",
    "    \n",
    "all_users=users.map(rep).filter(lambda x: x != 'Empty').sortBy(lambda x: -x[1])\n",
    "\n",
    "##get post with questions and answers\n",
    "def post(line):\n",
    "    try:\n",
    "        root = ET.fromstring(line)\n",
    "        if \"PostTypeId\" and \"OwnerUserId\" in root.attrib:\n",
    "            uid = root.attrib['OwnerUserId']\n",
    "            ptype = int(root.attrib['PostTypeId'])\n",
    "            if ptype == 1 or ptype == 2:\n",
    "                return (uid,ptype)\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        else:\n",
    "            return ('Empty')\n",
    "    except:\n",
    "        return ('Empty')\n",
    "    \n",
    "all_p = posts.map(post).filter(lambda x: x != 'Empty')\n",
    "\n",
    "questions = all_p.filter(lambda x: x[1] == 1).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "answers = all_p.filter(lambda x: x[1] == 2).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0],int(x[1]/2)))\n",
    "\n",
    "##merge \n",
    "answer_percentage_long=all_users.join(questions).join(answers)\\\n",
    "                                .map(lambda x: (int(x[0]),x[1][0][0],x[1][0][1],x[1][1]))\\\n",
    "                                .map(lambda x: (x[0],x[1],x[3]/(x[2]+x[3])))\\\n",
    "                                .takeOrdered(99, key=lambda x: -x[1])\n",
    "\n",
    "answer_percentage=[(x[0],x[2]) for x in answer_percentage_long]\n",
    "\n",
    "##calculate average\n",
    "total, count = all_users.join(questions).join(answers).map(lambda x: (int(x[0]),x[1][0][0],x[1][0][1],x[1][1]))\\\n",
    "                        .map(lambda x: (x[0],x[3]/(x[2]+x[3]),1))\\\n",
    "                        .map(lambda x: (x[1],x[2]))\\\n",
    "                        .reduce(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "\n",
    "avg = (-1,total/count)\n",
    "answer_percentage.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(919, 0.996694214876033),\n",
       " (805, 0.9959749552772809),\n",
       " (686, 0.9803049555273189),\n",
       " (7290, 0.9918887601390498),\n",
       " (930, 0.9817351598173516),\n",
       " (4505, 1.0),\n",
       " (4253, 0.9909747292418772),\n",
       " (183, 0.847870182555781),\n",
       " (11032, 0.9875647668393782),\n",
       " (28746, 0.968421052631579),\n",
       " (887, 0.9794871794871794),\n",
       " (159, 0.9728813559322034),\n",
       " (2116, 0.9833333333333333),\n",
       " (4856, 0.9543147208121827),\n",
       " (22047, 1.0),\n",
       " (5739, 0.9872773536895675),\n",
       " (3277, 0.956081081081081),\n",
       " (88, 0.9660493827160493),\n",
       " (2970, 1.0),\n",
       " (601, 0.9772151898734177),\n",
       " (17230, 0.9970059880239521),\n",
       " (449, 1.0),\n",
       " (2392, 0.9724137931034482),\n",
       " (1390, 0.9411764705882353),\n",
       " (5836, 0.846441947565543),\n",
       " (7555, 1.0),\n",
       " (603, 0.8158844765342961),\n",
       " (7972, 0.9823008849557522),\n",
       " (6633, 0.9912280701754386),\n",
       " (2958, 0.9930313588850174),\n",
       " (9394, 0.9700854700854701),\n",
       " (7828, 0.9850427350427351),\n",
       " (2817, 0.8206896551724138),\n",
       " (7224, 0.9757575757575757),\n",
       " (4598, 0.9857142857142858),\n",
       " (7071, 0.9107142857142857),\n",
       " (1739, 0.9948717948717949),\n",
       " (1036, 0.9545454545454546),\n",
       " (3382, 1.0),\n",
       " (8013, 0.9040697674418605),\n",
       " (3019, 0.8571428571428571),\n",
       " (4376, 0.963302752293578),\n",
       " (251, 0.9924242424242424),\n",
       " (28666, 0.9),\n",
       " (1764, 0.9325842696629213),\n",
       " (23853, 1.0),\n",
       " (32036, 0.9959839357429718),\n",
       " (10849, 0.9518072289156626),\n",
       " (26338, 0.9691358024691358),\n",
       " (1352, 0.9902912621359223),\n",
       " (401, 0.9119496855345912),\n",
       " (5, 0.8547008547008547),\n",
       " (8, 0.8991596638655462),\n",
       " (7250, 0.9877300613496932),\n",
       " (1909, 0.9518072289156626),\n",
       " (21054, 0.9345794392523364),\n",
       " (4257, 0.9757575757575757),\n",
       " (196, 0.7357512953367875),\n",
       " (442, 0.8712121212121212),\n",
       " (279, 1.0),\n",
       " (2669, 0.946843853820598),\n",
       " (8402, 0.6521739130434783),\n",
       " (36041, 0.9889807162534435),\n",
       " (2126, 1.0),\n",
       " (44269, 0.9033613445378151),\n",
       " (6029, 1.0),\n",
       " (11981, 0.9649122807017544),\n",
       " (1934, 0.9680851063829787),\n",
       " (795, 0.676056338028169),\n",
       " (25433, 0.9867256637168141),\n",
       " (253, 0.3695652173913043),\n",
       " (364, 0.6736111111111112),\n",
       " (25, 0.9166666666666666),\n",
       " (22311, 0.9401709401709402),\n",
       " (334, 1.0),\n",
       " (13047, 0.9733333333333334),\n",
       " (8507, 0.9428571428571428),\n",
       " (264, 0.90625),\n",
       " (14188, 0.8983050847457628),\n",
       " (307, 1.0),\n",
       " (8076, 0.9333333333333333),\n",
       " (5862, 1.0),\n",
       " (8413, 0.9836065573770492),\n",
       " (1307, 0.8333333333333334),\n",
       " (2860, 0.8903225806451613),\n",
       " (223, 0.8588235294117647),\n",
       " (11887, 0.976878612716763),\n",
       " (52554, 0.9652777777777778),\n",
       " (2074, 1.0),\n",
       " (35989, 0.9487179487179487),\n",
       " (1005, 0.0034482758620689655),\n",
       " (22228, 0.8513513513513513),\n",
       " (4862, 0.8974358974358975),\n",
       " (3601, 0.9852941176470589),\n",
       " (17908, 1.0),\n",
       " (13138, 0.8639455782312925),\n",
       " (1108, 0.9722222222222222),\n",
       " (1679, 0.941747572815534),\n",
       " (11852, 1.0),\n",
       " (-1, 0.19993935929508744)]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## First question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'd expect the first **question** a user asks to be indicative of their future behavior.  We'll dig more into that in the next problem, but for now let's see the relationship between reputation and how long it took each person to ask their first question.\n",
    "\n",
    "For each user that asked a question, find the difference between when their account was created (`CreationDate` for the User) and when they asked their first question (`CreationDate` for their first question).  Return this time difference in days (round down, so 2.7 days counts as 2 days) for the 100 users with the highest reputation, in the form\n",
    "\n",
    "`(UserId, Days)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_question(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"Id\" and \"Reputation\" and 'CreationDate' in root.attrib:\n",
    "                uid = root.attrib['Id']\n",
    "                rep = root.attrib['Reputation']\n",
    "                cdate = root.attrib['CreationDate']\n",
    "                return (int(uid),int(rep),cdate)\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsetime(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2010, 7, 19, 6, 55, 26, 860000)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsetime('2010-07-19T06:55:26.860')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsetime('2010-07-19T06:55:26.860')<parsetime('2010-07-20T06:55:26.860')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(parsetime('2010-07-19T06:55:26.860')-parsetime('2010-07-20T06:55:26.860')).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_info = users.map(first_question).filter(lambda x: x != 'Empty')\\\n",
    "                 .map(lambda x: (x[0], (x[1], parsetime(x[2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_first_question(line):\n",
    "    try:\n",
    "        root = ET.fromstring(line)\n",
    "        if \"PostTypeId\" and \"OwnerUserId\" and 'CreationDate' in root.attrib:\n",
    "            uid = root.attrib['OwnerUserId']\n",
    "            ptype = int(root.attrib['PostTypeId'])\n",
    "            cdate = root.attrib['CreationDate']\n",
    "            if ptype == 1:\n",
    "                return (int(uid),cdate)\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        else:\n",
    "            return ('Empty')\n",
    "    except:\n",
    "        return ('Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_info = posts.map(post_first_question).filter(lambda x: x != 'Empty')\\\n",
    "                 .map(lambda x: (x[0], parsetime(x[1])))\\\n",
    "                 .reduceByKey(lambda x, y: min(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, (1, datetime.datetime(2010, 7, 19, 6, 55, 26, 860000))),\n",
       " (2, (101, datetime.datetime(2010, 7, 19, 14, 1, 36, 697000))),\n",
       " (3, (101, datetime.datetime(2010, 7, 19, 15, 34, 50, 507000))),\n",
       " (4, (101, datetime.datetime(2010, 7, 19, 19, 3, 27, 400000))),\n",
       " (5, (6962, datetime.datetime(2010, 7, 19, 19, 3, 57, 227000)))]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32010, datetime.datetime(2013, 10, 28, 18, 36, 54, 563000)),\n",
       " (32021, datetime.datetime(2013, 10, 29, 0, 6, 34, 723000)),\n",
       " (26356, datetime.datetime(2013, 5, 31, 21, 17, 53, 117000)),\n",
       " (5676, datetime.datetime(2011, 8, 4, 3, 0, 9, 277000)),\n",
       " (32087, datetime.datetime(2013, 10, 30, 9, 32, 23, 277000))]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_info.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_delta(x,y):\n",
    "    if x is None or y is None:\n",
    "        return None\n",
    "    else:\n",
    "        return (x-y).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_delta(parsetime('2010-07-19T06:55:26.860'),parsetime('2010-07-20T06:55:26.860'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13,\n",
       "  947,\n",
       "  datetime.datetime(2010, 7, 19, 19, 6, 49, 527000),\n",
       "  datetime.datetime(2010, 7, 19, 19, 28, 44, 903000)),\n",
       " (26,\n",
       "  3220,\n",
       "  datetime.datetime(2010, 7, 19, 19, 9, 39, 723000),\n",
       "  datetime.datetime(2011, 2, 18, 2, 40, 12, 390000)),\n",
       " (39,\n",
       "  1741,\n",
       "  datetime.datetime(2010, 7, 19, 19, 11, 59, 377000),\n",
       "  datetime.datetime(2010, 7, 19, 20, 54, 23, 200000)),\n",
       " (52,\n",
       "  976,\n",
       "  datetime.datetime(2010, 7, 19, 19, 15, 5, 810000),\n",
       "  datetime.datetime(2010, 12, 5, 23, 43, 10, 43000)),\n",
       " (78,\n",
       "  123,\n",
       "  datetime.datetime(2010, 7, 19, 19, 26, 51, 850000),\n",
       "  datetime.datetime(2010, 8, 18, 22, 58, 5, 27000))]"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info.join(post_info).map(lambda x: (x[0],x[1][0][0],x[1][0][1],x[1][1])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_questions = user_info.join(post_info).map(lambda x: (x[0],x[1][0][0],x[1][0][1],x[1][1]))\\\n",
    ".map(lambda x:(x[0],x[1],time_delta(x[3],x[2]))).takeOrdered(100, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_question=[(x[0],x[2]) for x in first_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.9800000000000006\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#first_question = [(805, 669)] * 100\n",
    "\n",
    "grader.score('spark__first_question', first_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Identify veterans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "*Consider*: What other parameterizations of \"activity\" could we use, and how would they differ in terms of splitting our user base?\n",
    "\n",
    "*Consider*: What other biases are still not dealt with, after using the above approach?\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, average the score, views, number of answers, and number of favorites of the users' **first question**.\n",
    "\n",
    "*Consider*: What story could you tell from these numbers? How do the numbers support it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##find files\n",
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)\n",
    "\n",
    "posts = sc.textFile(localpath('spark-stats-data/allPosts'))\n",
    "users = sc.textFile(localpath('spark-stats-data/allUsers'))\n",
    "\n",
    "#define functions\n",
    "def parsetime(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%f')\n",
    "    \n",
    "def time_delta(x,y):\n",
    "    if x is None or y is None:\n",
    "        return None\n",
    "    else:\n",
    "        return (x-y).days\n",
    "    \n",
    "def user_parse(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"Id\" in root.attrib:\n",
    "                uid = root.attrib['Id']\n",
    "                cdate = root.attrib.get('CreationDate',None)\n",
    "                return (int(uid),cdate)\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')\n",
    "    \n",
    "def date_post(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"OwnerUserId\" in root.attrib:\n",
    "                uid = root.attrib['OwnerUserId']\n",
    "                cdate = root.attrib.get('CreationDate',None)\n",
    "                return (int(uid),cdate)\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')\n",
    "    \n",
    "def vet(x):\n",
    "    if x is None:\n",
    "        return 0\n",
    "    if int(x) >= 100 and int(x) <= 150:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "##get rdds\n",
    "valid_users = users.map(user_parse).filter(lambda x: x != 'Empty').map(lambda x: (x[0],parsetime(x[1])))    \n",
    "\n",
    "valid_posts = posts.map(date_post).filter(lambda x: x != 'Empty')\\\n",
    "                        .map(lambda x: (x[0],parsetime(x[1])))\n",
    "\n",
    "all_user_info = valid_users.join(valid_posts)\\\n",
    "                           .map(lambda x: (x[0],time_delta(x[1][1],x[1][0])))         \n",
    "\n",
    "all_i = all_user_info.map(lambda x: (x[0],vet(x[1]))).reduceByKey(lambda x,y: max(x,y))\n",
    "\n",
    "veteran = all_i.filter(lambda x: x[1] == 1)\n",
    "brief_views = all_i.filter(lambda x: x[1] == 0)\n",
    "\n",
    "def post_first_question_more(line):\n",
    "    try:\n",
    "        root = ET.fromstring(line)\n",
    "        if \"PostTypeId\" and \"OwnerUserId\" and 'CreationDate' in root.attrib:\n",
    "            uid = root.attrib['OwnerUserId']\n",
    "            ptype = int(root.attrib['PostTypeId'])\n",
    "            cdate = root.attrib['CreationDate']\n",
    "            #a.attrib.get('Score',1)\n",
    "            answer = root.attrib.get(\"AnswerCount\",0)           \n",
    "            view = root.attrib.get(\"ViewCount\",0)           \n",
    "            fav = root.attrib.get(\"FavoriteCount\",0)            \n",
    "            s = root.attrib.get(\"Score\",0) \n",
    "            #filter out questions\n",
    "            if ptype == 1:\n",
    "                return (int(uid),cdate,int(answer),int(view),int(fav),int(s))\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        else:\n",
    "            return ('Empty')\n",
    "    except:\n",
    "        return ('Empty')\n",
    "\n",
    "#find out the first questions date\n",
    "first_q= posts_long.map(post_first_question_more).filter(lambda x: x != 'Empty')\\\n",
    "              .map(lambda x: (x[0], parsetime(x[1])))\\\n",
    "              .reduceByKey(lambda x, y: min(x, y))\\\n",
    "              .map(lambda x: ((x[0],x[1]),1))\n",
    "\n",
    "#find out all questions' performance\n",
    "all_q = posts_long.map(post_first_question_more).filter(lambda x: x != 'Empty')\\\n",
    "              .map(lambda x: ((x[0],parsetime(x[1])),(x[2],x[3],x[4],x[5])))\n",
    "\n",
    "#merge (acount, first_question_date) with performances\n",
    "stats = first_q.join(all_q).map(lambda x: (x[0][0],(x[1][1][0],x[1][1][1],x[1][1][2],x[1][1][3])))\n",
    "\n",
    "##calculate average for veterian\n",
    "a,v,f,s,t=veteran.join(stats).map(lambda x: (x[0],(x[1][1][0],x[1][1][1],x[1][1][2],x[1][1][3])))\\\n",
    "                         .map(lambda x: (x[1][0],x[1][1],x[1][2],x[1][3],1))\\\n",
    "                         .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1],x[2]+y[2],x[3]+y[3],x[4]+y[4]))       \n",
    "\n",
    "###calculate average for brief views\n",
    "a1,v1,f1,s1,t1=brief_views.join(stats).map(lambda x: (x[0],(x[1][1][0],x[1][1][1],x[1][1][2],x[1][1][3])))\\\n",
    "                         .map(lambda x: (x[1][0],x[1][1],x[1][2],x[1][3],1))\\\n",
    "                         .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1],x[2]+y[2],x[3]+y[3],x[4]+y[4]))       \n",
    "\n",
    "#pass to lists\n",
    "lis=[s,v,a,f]\n",
    "lis1=[l/t for l in lis]\n",
    "\n",
    "lis=[s1,v1,a1,f1]\n",
    "lis2=[l/t1 for l in lis]\n",
    "\n",
    "lis1.extend(lis2)\n",
    "\n",
    "names=[\"vet_score\",\"vet_views\",\"vet_answers\",\"vet_favorites\",\"brief_score\",\"brief_views\",\"brief_answers\",\"brief_favorites\"]\n",
    "identify_veterans_full = {n:l for n,l in zip(names,lis1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Identify veterans&mdash;full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Same as above, but on the full Stack Exchange data set.\n",
    "\n",
    "No pre-parsed data is available for this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)\n",
    "\n",
    "posts_long = sc.textFile(localpath('spark-stack-data/allPosts'))\n",
    "users_long = sc.textFile(localpath('spark-stack-data/allUsers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsetime(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.strptime(x,'%Y-%m-%dT%H:%M:%S.%f')\n",
    "    \n",
    "def time_delta(x,y):\n",
    "    if x is None or y is None:\n",
    "        return None\n",
    "    else:\n",
    "        return (x-y).days\n",
    "    \n",
    "def user_parse(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"Id\" in root.attrib:\n",
    "                uid = root.attrib['Id']\n",
    "                cdate = root.attrib.get('CreationDate',None)\n",
    "                return (int(uid),cdate)\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')\n",
    "    \n",
    "def date_post(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = ET.fromstring(line)\n",
    "            if \"OwnerUserId\" in root.attrib:\n",
    "                uid = root.attrib['OwnerUserId']\n",
    "                cdate = root.attrib.get('CreationDate',None)\n",
    "                return (int(uid),cdate)\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')\n",
    "    \n",
    "def vet(x):\n",
    "    if x is None:\n",
    "        return 0\n",
    "    if int(x) >= 100 and int(x) <= 150:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "valid_users = users_long.map(user_parse).filter(lambda x: x != 'Empty').map(lambda x: (x[0],parsetime(x[1])))    \n",
    "\n",
    "valid_posts = posts_long.map(date_post).filter(lambda x: x != 'Empty')\\\n",
    "                        .map(lambda x: (x[0],parsetime(x[1])))\n",
    "\n",
    "all_user_info = valid_users.join(valid_posts)\\\n",
    "                           .map(lambda x: (x[0],time_delta(x[1][1],x[1][0])))         \n",
    "\n",
    "all_i = all_user_info.map(lambda x: (x[0],vet(x[1]))).reduceByKey(lambda x,y: max(x,y))\n",
    "\n",
    "veteran = all_i.filter(lambda x: x[1] == 1)\n",
    "brief_views = all_i.filter(lambda x: x[1] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291323"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veteran.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1843079"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_views.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_first_question_more(line):\n",
    "    try:\n",
    "        root = ET.fromstring(line)\n",
    "        if \"PostTypeId\" and \"OwnerUserId\" and 'CreationDate' in root.attrib:\n",
    "            uid = root.attrib['OwnerUserId']\n",
    "            ptype = int(root.attrib['PostTypeId'])\n",
    "            cdate = root.attrib['CreationDate']\n",
    "            #a.attrib.get('Score',1)\n",
    "            answer = root.attrib.get(\"AnswerCount\",0)           \n",
    "            view = root.attrib.get(\"ViewCount\",0)           \n",
    "            fav = root.attrib.get(\"FavoriteCount\",0)            \n",
    "            s = root.attrib.get(\"Score\",0) \n",
    "            #filter out questions\n",
    "            if ptype == 1:\n",
    "                return (int(uid),cdate,int(answer),int(view),int(fav),int(s))\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        else:\n",
    "            return ('Empty')\n",
    "    except:\n",
    "        return ('Empty')\n",
    "\n",
    "#find out the first questions date\n",
    "first_q= posts_long.map(post_first_question_more).filter(lambda x: x != 'Empty')\\\n",
    "              .map(lambda x: (x[0], parsetime(x[1])))\\\n",
    "              .reduceByKey(lambda x, y: min(x, y))\\\n",
    "              .map(lambda x: ((x[0],x[1]),1))\n",
    "\n",
    "#find out all questions' performance\n",
    "all_q = posts_long.map(post_first_question_more).filter(lambda x: x != 'Empty')\\\n",
    "              .map(lambda x: ((x[0],parsetime(x[1])),(x[2],x[3],x[4],x[5])))\n",
    "\n",
    "#merge (acount, first_question_date) with performances\n",
    "stats = first_q.join(all_q).map(lambda x: (x[0][0],(x[1][1][0],x[1][1][1],x[1][1][2],x[1][1][3])))\n",
    "\n",
    "##calculate average for veterian\n",
    "a,v,f,s,t=veteran.join(stats).map(lambda x: (x[0],(x[1][1][0],x[1][1][1],x[1][1][2],x[1][1][3])))\\\n",
    "                         .map(lambda x: (x[1][0],x[1][1],x[1][2],x[1][3],1))\\\n",
    "                         .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1],x[2]+y[2],x[3]+y[3],x[4]+y[4]))       \n",
    "\n",
    "###calculate average for brief views\n",
    "a1,v1,f1,s1,t1=brief_views.join(stats).map(lambda x: (x[0],(x[1][1][0],x[1][1][1],x[1][1][2],x[1][1][3])))\\\n",
    "                         .map(lambda x: (x[1][0],x[1][1],x[1][2],x[1][3],1))\\\n",
    "                         .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1],x[2]+y[2],x[3]+y[3],x[4]+y[4]))       \n",
    "\n",
    "#pass to lists\n",
    "lis=[s,v,a,f]\n",
    "lis1=[l/t for l in lis]\n",
    "\n",
    "lis=[s1,v1,a1,f1]\n",
    "lis2=[l/t1 for l in lis]\n",
    "\n",
    "lis1.extend(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2561266751584585,\n",
       " 1841.8189951718825,\n",
       " 1.841804785404288,\n",
       " 0.8661474978452715,\n",
       " 1.129298016272707,\n",
       " 1095.1468872554271,\n",
       " 1.5033659787554736,\n",
       " 0.3854817164205759]"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identify_veterans_full = {n:l for n,l in zip(names,lis1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Word2Vec is an alternative approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the tags of each Stack Exchange post as documents (this uses the full data set). Use the implementation of Word2Vec from Spark ML (this will require using DataFrames) to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The dimensionality of the vector space should be 100. The random seed should be 42 in `PySpark`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Mean of the top 25 cosine similarities: 0.8012362027168274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import re\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from lxml.etree import XMLSyntaxError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = sc.textFile(localpath('spark-stack-data/allPosts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_post(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = etree.fromstring(line)\n",
    "            if \"Tags\" in root.attrib:\n",
    "                tag = root.attrib['Tags']\n",
    "                return tag\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except XMLSyntaxError:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_posts = posts.map(tag_post).filter(lambda x: x!= 'Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<android><android-tabhost><progressdialog>',\n",
       " '<security><encryption><hash><passwords><password-protection>',\n",
       " '<java><proxy><hostname><resolve>',\n",
       " '<sql><jpa>',\n",
       " '<jquery><class>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(10)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(word='lattice', similarity=0.894500195980072), Row(word='r-grid', similarity=0.8603865504264832), Row(word='boxplot', similarity=0.8382558822631836), Row(word='plotrix', similarity=0.8369892239570618), Row(word='density-plot', similarity=0.8145373463630676), Row(word='ecdf', similarity=0.8129977583885193), Row(word='ggvis', similarity=0.805046796798706), Row(word='gridextra', similarity=0.8037905693054199), Row(word='levelplot', similarity=0.7975961565971375), Row(word='tapply', similarity=0.7965813875198364), Row(word='rgl', similarity=0.7900989055633545), Row(word='r-raster', similarity=0.7896594405174255), Row(word='quantile', similarity=0.7871367335319519), Row(word='r-factor', similarity=0.7825618386268616), Row(word='plot', similarity=0.7801932096481323), Row(word='gam', similarity=0.7788233757019043), Row(word='anova', similarity=0.7784751057624817), Row(word='confidence-interval', similarity=0.7770544290542603), Row(word='plotmath', similarity=0.7767807841300964), Row(word='line-plot', similarity=0.7740892767906189), Row(word='do.call', similarity=0.7739959955215454), Row(word='standard-error', similarity=0.7729793190956116), Row(word='data.table', similarity=0.7726537585258484), Row(word='performanceanalytics', similarity=0.7720458507537842), Row(word='kernel-density', similarity=0.770494818687439)]\n"
     ]
    }
   ],
   "source": [
    "df=tag_posts.map(lambda line: ([s for s in re.split(\"<|>\", line) if s != ''], 1))\\\n",
    "            .toDF(['text', 'score'])\n",
    "\n",
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\", vectorSize=100,minCount=10,seed=17)\n",
    "model = w2v.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "print(model.findSynonyms('ggplot2', 25).rdd.take(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lm', 0.8915739059448242)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.findSynonyms('ggplot2', 25).rdd.take(25)[0]['word'],model.findSynonyms('ggplot2', 25).rdd.take(25)[0]['similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lattice', 0.894500195980072),\n",
       " ('r-grid', 0.8603865504264832),\n",
       " ('boxplot', 0.8382558822631836),\n",
       " ('plotrix', 0.8369892239570618),\n",
       " ('density-plot', 0.8145373463630676),\n",
       " ('ecdf', 0.8129977583885193),\n",
       " ('ggvis', 0.805046796798706),\n",
       " ('gridextra', 0.8037905693054199),\n",
       " ('levelplot', 0.7975961565971375),\n",
       " ('tapply', 0.7965813875198364),\n",
       " ('rgl', 0.7900989055633545),\n",
       " ('r-raster', 0.7896594405174255),\n",
       " ('quantile', 0.7871367335319519),\n",
       " ('r-factor', 0.7825618386268616),\n",
       " ('plot', 0.7801932096481323),\n",
       " ('gam', 0.7788233757019043),\n",
       " ('anova', 0.7784751057624817),\n",
       " ('confidence-interval', 0.7770544290542603),\n",
       " ('plotmath', 0.7767807841300964),\n",
       " ('line-plot', 0.7740892767906189),\n",
       " ('do.call', 0.7739959955215454),\n",
       " ('standard-error', 0.7729793190956116),\n",
       " ('data.table', 0.7726537585258484),\n",
       " ('performanceanalytics', 0.7720458507537842),\n",
       " ('kernel-density', 0.770494818687439)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = model.findSynonyms('ggplot2', 25).rdd.take(25)\n",
    "word2vec = [(l['word'],l['similarity']) for l in lis]\n",
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\",minCount=10, vectorSize=100,seed=17)\n",
    "model = w2v.fit(df)\n",
    "result = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                text|score|             vectors|\n",
      "+--------------------+-----+--------------------+\n",
      "|[ranking, rank-co...|    1|[0.00461881561204...|\n",
      "|[data-visualization]|    1|[-0.1391233950853...|\n",
      "|[probability, mix...|    1|[-0.0312058134004...|\n",
      "| [r, bioinformatics]|    1|[-0.0321438718237...|\n",
      "|[machine-learning...|    1|[0.11158403381705...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                word|        similarity|\n",
      "+--------------------+------------------+\n",
      "|                  lm|0.8915739059448242|\n",
      "|     beta-regression|0.8643296360969543|\n",
      "|                 plm| 0.857779324054718|\n",
      "|robust-standard-e...|0.8324904441833496|\n",
      "|           dataframe|0.8204379677772522|\n",
      "|              mlogit|0.8197407126426697|\n",
      "|                 gam|0.8076692819595337|\n",
      "|                 nls|  0.80257648229599|\n",
      "| stepwise-regression|0.7994891405105591|\n",
      "|       error-message|0.7964061498641968|\n",
      "|             splines|0.7962846755981445|\n",
      "|       ordered-logit|0.7921024560928345|\n",
      "|               loess|0.7862126231193542|\n",
      "|            traminer|0.7547711730003357|\n",
      "|                nlme|0.7518925666809082|\n",
      "|                 gis|0.7484728097915649|\n",
      "|            survival|0.7422318458557129|\n",
      "|           intercept|0.7362070083618164|\n",
      "|                lmer|0.7275500893592834|\n",
      "|             hausman|0.7262603044509888|\n",
      "|        longitudinal| 0.721697211265564|\n",
      "|           cox-model|0.7216238975524902|\n",
      "|difference-in-dif...|0.7190772294998169|\n",
      "|         scatterplot|0.7168862223625183|\n",
      "|                lme4|0.7124378085136414|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms('ggplot2', 25).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'd like to see if we can predict the tags of a question from its body text. Instead of predicting specific tags, we will instead try to predict if a question contains one of the top ten most common tags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = sc.textFile(localpath('spark-stats-data/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_tag(string):\n",
    "    tokens= [s for s in re.split(\"<|>\", string) if s != '']\n",
    "    token_count=[(t,1) for t in tokens]\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '<bayesian><prior><elicitation>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bayesian', 1), ('prior', 1), ('elicitation', 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_tag(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_tags = train.map(tag_post).filter(lambda x: x != 'Empty')\\\n",
    "                   .flatMap(token_tag).reduceByKey(lambda x,y: x+y)\\\n",
    "                   .takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r',\n",
       " 'regression',\n",
       " 'time-series',\n",
       " 'machine-learning',\n",
       " 'probability',\n",
       " 'hypothesis-testing',\n",
       " 'distributions',\n",
       " 'self-study',\n",
       " 'logistic',\n",
       " 'correlation']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [t[0] for t in common_tags]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_check(string):\n",
    "    tokens= [s for s in re.split(\"<|>\", string) if s != '']\n",
    "    for t in tags:\n",
    "        if t in tokens:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string ='<regression><tag>'\n",
    "token_check(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile('<p>(.+)</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile('>(.+)<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O\\'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Machine Learning, fight!\"</a> that discussed some of the differences between the two fields.  <a href=\"http://andrewgelman.com/2008/12/machine_learnin/\">Andrew Gelman responded favorably to this</a>:Simon Blomberg: Andrew Gelman:There was also the <a href=\"http://projecteuclid.org/euclid.ss/1009213726\"><strong>\"Statistical Modeling: The Two Cultures\"</strong> paper</a> by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the <em>predictive accuracy</em> of models.Has the statistics field changed over the last decade in response to these critiques?  Do the <em>two cultures</em> still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '<p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O\\'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Machine Learning, fight!\"</a> that discussed some of the differences between the two fields.  <a href=\"http://andrewgelman.com/2008/12/machine_learnin/\">Andrew Gelman responded favorably to this</a>:</p>\\n\\n<p>Simon Blomberg: </p>\\n\\n<blockquote>\\n  <p>From R\\'s fortunes\\n  package: To paraphrase provocatively,\\n  \\'machine learning is statistics minus\\n  any checking of models and\\n  assumptions\\'.\\n  -- Brian D. Ripley (about the difference between machine learning\\n  and statistics) useR! 2004, Vienna\\n  (May 2004) :-) Season\\'s Greetings!</p>\\n</blockquote>\\n\\n<p>Andrew Gelman:</p>\\n\\n<blockquote>\\n  <p>In that case, maybe we should get rid\\n  of checking of models and assumptions\\n  more often. Then maybe we\\'d be able to\\n  solve some of the problems that the\\n  machine learning people can solve but\\n  we can\\'t!</p>\\n</blockquote>\\n\\n<p>There was also the <a href=\"http://projecteuclid.org/euclid.ss/1009213726\"><strong>\"Statistical Modeling: The Two Cultures\"</strong> paper</a> by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the <em>predictive accuracy</em> of models.</p>\\n\\n<p>Has the statistics field changed over the last decade in response to these critiques?  Do the <em>two cultures</em> still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?</p>\\n'\n",
    "''.join(p.findall(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_body(string):\n",
    "    p = re.compile('<p>(.+)</p>')\n",
    "    lis = p.findall(string)\n",
    "    return ''.join(p.findall(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_post(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = etree.fromstring(line)\n",
    "            if 'Body' and \"Tags\" in root.attrib:\n",
    "                body = root.attrib['Body']\n",
    "                tag = root.attrib['Tags']\n",
    "                return (body,token_check(tag))\n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except XMLSyntaxError:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<p>How should I elicit prior distributions from experts when fitting a Bayesian model?</p>\\n',\n",
       "  0),\n",
       " ('<p>In many different statistical methods there is an \"assumption of normality\".  What is \"normality\" and how do I know if there is normality?</p>\\n',\n",
       "  1),\n",
       " ('<p>What are some valuable Statistical Analysis open source projects available right now?</p>\\n\\n<p>Edit: as pointed out by Sharpie, valuable could mean helping you get things done faster or more cheaply.</p>\\n',\n",
       "  0),\n",
       " (\"<p>I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  </p>\\n\\n<p>What tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?</p>\\n\",\n",
       "  1),\n",
       " ('<p>Last year, I read a blog post from <a href=\"http://anyall.org/\">Brendan O\\'Connor</a> entitled <a href=\"http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\">\"Statistics vs. Machine Learning, fight!\"</a> that discussed some of the differences between the two fields.  <a href=\"http://andrewgelman.com/2008/12/machine_learnin/\">Andrew Gelman responded favorably to this</a>:</p>\\n\\n<p>Simon Blomberg: </p>\\n\\n<blockquote>\\n  <p>From R\\'s fortunes\\n  package: To paraphrase provocatively,\\n  \\'machine learning is statistics minus\\n  any checking of models and\\n  assumptions\\'.\\n  -- Brian D. Ripley (about the difference between machine learning\\n  and statistics) useR! 2004, Vienna\\n  (May 2004) :-) Season\\'s Greetings!</p>\\n</blockquote>\\n\\n<p>Andrew Gelman:</p>\\n\\n<blockquote>\\n  <p>In that case, maybe we should get rid\\n  of checking of models and assumptions\\n  more often. Then maybe we\\'d be able to\\n  solve some of the problems that the\\n  machine learning people can solve but\\n  we can\\'t!</p>\\n</blockquote>\\n\\n<p>There was also the <a href=\"http://projecteuclid.org/euclid.ss/1009213726\"><strong>\"Statistical Modeling: The Two Cultures\"</strong> paper</a> by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the <em>predictive accuracy</em> of models.</p>\\n\\n<p>Has the statistics field changed over the last decade in response to these critiques?  Do the <em>two cultures</em> still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?</p>\\n',\n",
       "  1)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.map(tag_post).filter(lambda x: x != 'Empty').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = train.map(tag_post).filter(lambda x: x != 'Empty').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19540"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.map(tag_post).filter(lambda x: x != 'Empty').filter(lambda x: x[1]==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer,RegexTokenizer\n",
    "##save the computation from the traing process by cache()\n",
    "training = sqlContext.createDataFrame(train_set, [\"title\", \"label\"]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|label|\n",
      "+--------------------+-----+\n",
      "|<p>How should I e...|    0|\n",
      "|<p>In many differ...|    1|\n",
      "|<p>What are some ...|    0|\n",
      "|<p>I have two gro...|    1|\n",
      "|<p>Last year, I r...|    1|\n",
      "|<p>I've been work...|    0|\n",
      "|<p>Sorry, but the...|    0|\n",
      "|<p>Many studies i...|    0|\n",
      "|<p>I have four co...|    0|\n",
      "|<p>What are some ...|    0|\n",
      "|<p>How would you ...|    0|\n",
      "|<p>How can I find...|    1|\n",
      "|<p>What modern to...|    1|\n",
      "|<p>What is a stan...|    0|\n",
      "|<p>Which methods ...|    1|\n",
      "|<p>After taking a...|    1|\n",
      "|<p>What R package...|    1|\n",
      "|<p>I have a data ...|    1|\n",
      "|<p>There is an ol...|    1|\n",
      "|<p>I'm looking fo...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"words\", pattern=\"\\\\w\")\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "logreg = LogisticRegression(maxIter=1000000, regParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, hashingTF, logreg])\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \n",
    "    .addGrid(hashingTF.numFeatures, [2000])\n",
    "    .addGrid(logreg.regParam, [10, 1, 0.1]) \n",
    "    .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = sc.textFile(localpath('spark-stats-data/test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19414"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_post_test(line):\n",
    "    if '<row' in line:\n",
    "        try:\n",
    "            root = etree.fromstring(line)\n",
    "            if 'Body' and 'Id' and \"PostTypeId\" in root.attrib:\n",
    "                if int(root.attrib['PostTypeId']) == 1:\n",
    "                    pid = root.attrib['Id']\n",
    "                    body = root.attrib['Body']\n",
    "                    return (body, int(pid))\n",
    "                else:\n",
    "                    return ('Empty') \n",
    "            else:\n",
    "                return ('Empty')\n",
    "        except XMLSyntaxError:\n",
    "            return ('Empty')\n",
    "    else:\n",
    "        return ('Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4649"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.map(tag_post_test).filter(lambda x: x != 'Empty').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = test.map(tag_post_test).filter(lambda x: x != 'Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = sqlContext.createDataFrame(test_set, [\"title\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               title| id|\n",
      "+--------------------+---+\n",
      "|<p>Is there a goo...| 11|\n",
      "|<p>What algorithm...| 40|\n",
      "|<p>I have a datas...| 47|\n",
      "|<p>We're trying t...| 93|\n",
      "|<p>I need to anal...|183|\n",
      "|<p>I have 2 ASR (...|212|\n",
      "|<p>What are some ...|216|\n",
      "|<p>I have a frien...|223|\n",
      "|<p>When a non-hie...|278|\n",
      "|<p>I know of Came...|290|\n",
      "|<p>I'm a physics ...|312|\n",
      "|<p>I realize that...|328|\n",
      "|<p>Why do we seek...|354|\n",
      "|<p>What is the di...|362|\n",
      "|<p>If you could g...|363|\n",
      "|<p>From Wikipedia...|373|\n",
      "|<p>I am proposing...|492|\n",
      "|<p>Sometimes, I j...|498|\n",
      "|<p>In answering <...|539|\n",
      "|<p>In engineering...|624|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "better_prediction = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|               title| id|               words|            filtered|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|<p>Is there a goo...| 11|[<p>is, there, a,...|[<p>is, good,, mo...|(2000,[139,255,27...|[0.77586480998409...|[0.68478820225451...|       0.0|\n",
      "|<p>What algorithm...| 40|[<p>what, algorit...|[<p>what, algorit...|(2000,[722,738,85...|[-0.0618889691333...|[0.48453269436504...|       1.0|\n",
      "|<p>I have a datas...| 47|[<p>i, have, a, d...|[<p>i, dataset, 1...|(2000,[3,11,43,80...|[-0.0133490083470...|[0.49666279746943...|       1.0|\n",
      "|<p>We're trying t...| 93|[<p>we're, trying...|[<p>we're, trying...|(2000,[77,101,138...|[0.47198871542085...|[0.61585435051996...|       0.0|\n",
      "|<p>I need to anal...|183|[<p>i, need, to, ...|[<p>i, need, anal...|(2000,[10,143,246...|[0.76637981459107...|[0.68273725515569...|       0.0|\n",
      "|<p>I have 2 ASR (...|212|[<p>i, have, 2, a...|[<p>i, 2, asr, (a...|(2000,[14,25,148,...|[0.18462071271251...|[0.54602452429413...|       0.0|\n",
      "|<p>What are some ...|216|[<p>what, are, so...|[<p>what, good, v...|(2000,[0,168,489,...|[0.09338754557696...|[0.52332993337490...|       0.0|\n",
      "|<p>I have a frien...|223|[<p>i, have, a, f...|[<p>i, friend, md...|(2000,[0,62,140,2...|[-0.1804401342411...|[0.45501196270476...|       1.0|\n",
      "|<p>When a non-hie...|278|[<p>when, a, non-...|[<p>when, non-hie...|(2000,[44,85,86,2...|[0.95249815432551...|[0.72161729941506...|       0.0|\n",
      "|<p>I know of Came...|290|[<p>i, know, of, ...|[<p>i, know, came...|(2000,[168,175,45...|[-0.1089466610843...|[0.47279024297369...|       1.0|\n",
      "|<p>I'm a physics ...|312|[<p>i'm, a, physi...|[<p>i'm, physics,...|(2000,[66,70,77,9...|[0.09692764236552...|[0.52421295689636...|       0.0|\n",
      "|<p>I realize that...|328|[<p>i, realize, t...|[<p>i, realize, s...|(2000,[50,99,114,...|[0.41003157774420...|[0.60109545057488...|       0.0|\n",
      "|<p>Why do we seek...|354|[<p>why, do, we, ...|[<p>why, seek, mi...|(2000,[128,140,17...|[0.08709251996551...|[0.52175937779568...|       0.0|\n",
      "|<p>What is the di...|362|[<p>what, is, the...|[<p>what, differe...|(2000,[379,501,53...|[0.06322057338276...|[0.51579988122858...|       0.0|\n",
      "|<p>If you could g...|363|[<p>if, you, coul...|[<p>if, go, back,...|(2000,[59,77,157,...|[0.01997788330002...|[0.50499430471727...|       0.0|\n",
      "|<p>From Wikipedia...|373|[<p>from, wikiped...|[<p>from, wikiped...|(2000,[3,44,92,16...|[-0.3340073768577...|[0.41726588730537...|       1.0|\n",
      "|<p>I am proposing...|492|[<p>i, am, propos...|[<p>i, proposing,...|(2000,[19,98,123,...|[0.00701784963473...|[0.50175445520808...|       0.0|\n",
      "|<p>Sometimes, I j...|498|[<p>sometimes,, i...|[<p>sometimes,, w...|(2000,[50,80,99,3...|[-0.0631147249433...|[0.48422655450185...|       1.0|\n",
      "|<p>In answering <...|539|[<p>in, answering...|[<p>in, answering...|(2000,[5,32,34,41...|[0.11533257122397...|[0.52880122467907...|       0.0|\n",
      "|<p>In engineering...|624|[<p>in, engineeri...|[<p>in, engineeri...|(2000,[20,188,300...|[0.09114783351438...|[0.52277119544091...|       0.0|\n",
      "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "better_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = better_prediction.select(\"id\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for row in selected.collect()[5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=11, prediction=0.0),\n",
       " Row(id=40, prediction=1.0),\n",
       " Row(id=47, prediction=1.0),\n",
       " Row(id=93, prediction=0.0),\n",
       " Row(id=183, prediction=0.0),\n",
       " Row(id=212, prediction=0.0),\n",
       " Row(id=216, prediction=0.0),\n",
       " Row(id=223, prediction=1.0),\n",
       " Row(id=278, prediction=0.0),\n",
       " Row(id=290, prediction=1.0),\n",
       " Row(id=312, prediction=0.0),\n",
       " Row(id=328, prediction=0.0),\n",
       " Row(id=354, prediction=0.0),\n",
       " Row(id=362, prediction=0.0),\n",
       " Row(id=363, prediction=0.0),\n",
       " Row(id=373, prediction=1.0),\n",
       " Row(id=492, prediction=0.0),\n",
       " Row(id=498, prediction=1.0),\n",
       " Row(id=539, prediction=0.0),\n",
       " Row(id=624, prediction=0.0),\n",
       " Row(id=841, prediction=0.0),\n",
       " Row(id=886, prediction=1.0),\n",
       " Row(id=897, prediction=1.0),\n",
       " Row(id=928, prediction=0.0),\n",
       " Row(id=929, prediction=0.0),\n",
       " Row(id=944, prediction=1.0),\n",
       " Row(id=946, prediction=1.0),\n",
       " Row(id=977, prediction=0.0),\n",
       " Row(id=1063, prediction=0.0),\n",
       " Row(id=1082, prediction=0.0),\n",
       " Row(id=1228, prediction=0.0),\n",
       " Row(id=1266, prediction=1.0),\n",
       " Row(id=1308, prediction=1.0),\n",
       " Row(id=1357, prediction=0.0),\n",
       " Row(id=1525, prediction=1.0),\n",
       " Row(id=1534, prediction=1.0),\n",
       " Row(id=1538, prediction=1.0),\n",
       " Row(id=1562, prediction=1.0),\n",
       " Row(id=1564, prediction=1.0),\n",
       " Row(id=1580, prediction=1.0),\n",
       " Row(id=1590, prediction=1.0),\n",
       " Row(id=1604, prediction=0.0),\n",
       " Row(id=1610, prediction=0.0),\n",
       " Row(id=1645, prediction=1.0),\n",
       " Row(id=1699, prediction=1.0),\n",
       " Row(id=1709, prediction=0.0),\n",
       " Row(id=1757, prediction=1.0),\n",
       " Row(id=1787, prediction=1.0),\n",
       " Row(id=1826, prediction=0.0),\n",
       " Row(id=1838, prediction=1.0),\n",
       " Row(id=1844, prediction=0.0),\n",
       " Row(id=1863, prediction=0.0),\n",
       " Row(id=1866, prediction=0.0),\n",
       " Row(id=1927, prediction=0.0),\n",
       " Row(id=2010, prediction=1.0),\n",
       " Row(id=2059, prediction=0.0),\n",
       " Row(id=2111, prediction=1.0),\n",
       " Row(id=2119, prediction=1.0),\n",
       " Row(id=2125, prediction=1.0),\n",
       " Row(id=2181, prediction=0.0),\n",
       " Row(id=2213, prediction=0.0),\n",
       " Row(id=2256, prediction=1.0),\n",
       " Row(id=2299, prediction=0.0),\n",
       " Row(id=2343, prediction=0.0),\n",
       " Row(id=2401, prediction=0.0),\n",
       " Row(id=2439, prediction=0.0),\n",
       " Row(id=2513, prediction=1.0),\n",
       " Row(id=2742, prediction=0.0),\n",
       " Row(id=2764, prediction=0.0),\n",
       " Row(id=2844, prediction=1.0),\n",
       " Row(id=2886, prediction=1.0),\n",
       " Row(id=2894, prediction=0.0),\n",
       " Row(id=2981, prediction=0.0),\n",
       " Row(id=3001, prediction=1.0),\n",
       " Row(id=3031, prediction=1.0),\n",
       " Row(id=3061, prediction=1.0),\n",
       " Row(id=3062, prediction=0.0),\n",
       " Row(id=3100, prediction=1.0),\n",
       " Row(id=3105, prediction=0.0),\n",
       " Row(id=3119, prediction=1.0),\n",
       " Row(id=3140, prediction=0.0),\n",
       " Row(id=3173, prediction=1.0),\n",
       " Row(id=3193, prediction=1.0),\n",
       " Row(id=3221, prediction=1.0),\n",
       " Row(id=3232, prediction=0.0),\n",
       " Row(id=3259, prediction=0.0),\n",
       " Row(id=3313, prediction=1.0),\n",
       " Row(id=3359, prediction=0.0),\n",
       " Row(id=3362, prediction=0.0),\n",
       " Row(id=3377, prediction=0.0),\n",
       " Row(id=3438, prediction=0.0),\n",
       " Row(id=3516, prediction=0.0),\n",
       " Row(id=3564, prediction=1.0),\n",
       " Row(id=3575, prediction=0.0),\n",
       " Row(id=3584, prediction=1.0),\n",
       " Row(id=3609, prediction=0.0),\n",
       " Row(id=3614, prediction=0.0),\n",
       " Row(id=3623, prediction=1.0),\n",
       " Row(id=3641, prediction=1.0),\n",
       " Row(id=3661, prediction=1.0),\n",
       " Row(id=3704, prediction=1.0),\n",
       " Row(id=3727, prediction=0.0),\n",
       " Row(id=3752, prediction=1.0),\n",
       " Row(id=3826, prediction=0.0),\n",
       " Row(id=3841, prediction=1.0),\n",
       " Row(id=3892, prediction=0.0),\n",
       " Row(id=3893, prediction=0.0),\n",
       " Row(id=3915, prediction=1.0),\n",
       " Row(id=4059, prediction=1.0),\n",
       " Row(id=4080, prediction=1.0),\n",
       " Row(id=4111, prediction=0.0),\n",
       " Row(id=4114, prediction=1.0),\n",
       " Row(id=4121, prediction=0.0),\n",
       " Row(id=4140, prediction=0.0),\n",
       " Row(id=4203, prediction=0.0),\n",
       " Row(id=4267, prediction=0.0),\n",
       " Row(id=4383, prediction=1.0),\n",
       " Row(id=4417, prediction=0.0),\n",
       " Row(id=4437, prediction=1.0),\n",
       " Row(id=4515, prediction=1.0),\n",
       " Row(id=4542, prediction=0.0),\n",
       " Row(id=4551, prediction=0.0),\n",
       " Row(id=4575, prediction=0.0),\n",
       " Row(id=4600, prediction=1.0),\n",
       " Row(id=4619, prediction=0.0),\n",
       " Row(id=4624, prediction=1.0),\n",
       " Row(id=4652, prediction=0.0),\n",
       " Row(id=4783, prediction=1.0),\n",
       " Row(id=4809, prediction=0.0),\n",
       " Row(id=4814, prediction=1.0),\n",
       " Row(id=4872, prediction=0.0),\n",
       " Row(id=4904, prediction=0.0),\n",
       " Row(id=4914, prediction=0.0),\n",
       " Row(id=4954, prediction=0.0),\n",
       " Row(id=4997, prediction=0.0),\n",
       " Row(id=5023, prediction=1.0),\n",
       " Row(id=5038, prediction=1.0),\n",
       " Row(id=5041, prediction=1.0),\n",
       " Row(id=5042, prediction=0.0),\n",
       " Row(id=5048, prediction=1.0),\n",
       " Row(id=5093, prediction=1.0),\n",
       " Row(id=5111, prediction=0.0),\n",
       " Row(id=5172, prediction=0.0),\n",
       " Row(id=5173, prediction=1.0),\n",
       " Row(id=5196, prediction=1.0),\n",
       " Row(id=5197, prediction=1.0),\n",
       " Row(id=5293, prediction=1.0),\n",
       " Row(id=5299, prediction=1.0),\n",
       " Row(id=5306, prediction=0.0),\n",
       " Row(id=5308, prediction=0.0),\n",
       " Row(id=5329, prediction=0.0),\n",
       " Row(id=5387, prediction=1.0),\n",
       " Row(id=5443, prediction=1.0),\n",
       " Row(id=5452, prediction=1.0),\n",
       " Row(id=5479, prediction=1.0),\n",
       " Row(id=5514, prediction=1.0),\n",
       " Row(id=5520, prediction=1.0),\n",
       " Row(id=5525, prediction=1.0),\n",
       " Row(id=5545, prediction=0.0),\n",
       " Row(id=5603, prediction=1.0),\n",
       " Row(id=5696, prediction=1.0),\n",
       " Row(id=5703, prediction=1.0),\n",
       " Row(id=5728, prediction=0.0),\n",
       " Row(id=5782, prediction=0.0),\n",
       " Row(id=5831, prediction=0.0),\n",
       " Row(id=5868, prediction=0.0),\n",
       " Row(id=5894, prediction=1.0),\n",
       " Row(id=5913, prediction=1.0),\n",
       " Row(id=5972, prediction=0.0),\n",
       " Row(id=6044, prediction=0.0),\n",
       " Row(id=6119, prediction=0.0),\n",
       " Row(id=6122, prediction=1.0),\n",
       " Row(id=6139, prediction=1.0),\n",
       " Row(id=6151, prediction=0.0),\n",
       " Row(id=6152, prediction=1.0),\n",
       " Row(id=6163, prediction=0.0),\n",
       " Row(id=6169, prediction=0.0),\n",
       " Row(id=6214, prediction=1.0),\n",
       " Row(id=6279, prediction=1.0),\n",
       " Row(id=6354, prediction=1.0),\n",
       " Row(id=6478, prediction=1.0),\n",
       " Row(id=6502, prediction=0.0),\n",
       " Row(id=6516, prediction=1.0),\n",
       " Row(id=6534, prediction=0.0),\n",
       " Row(id=6540, prediction=1.0),\n",
       " Row(id=6549, prediction=0.0),\n",
       " Row(id=6664, prediction=1.0),\n",
       " Row(id=6690, prediction=1.0),\n",
       " Row(id=6720, prediction=1.0),\n",
       " Row(id=6780, prediction=0.0),\n",
       " Row(id=6817, prediction=0.0),\n",
       " Row(id=6824, prediction=0.0),\n",
       " Row(id=6835, prediction=1.0),\n",
       " Row(id=6841, prediction=1.0),\n",
       " Row(id=6876, prediction=0.0),\n",
       " Row(id=6891, prediction=1.0),\n",
       " Row(id=6967, prediction=1.0),\n",
       " Row(id=6976, prediction=1.0),\n",
       " Row(id=6998, prediction=0.0),\n",
       " Row(id=7022, prediction=1.0),\n",
       " Row(id=7029, prediction=0.0),\n",
       " Row(id=7045, prediction=1.0),\n",
       " Row(id=7089, prediction=1.0),\n",
       " Row(id=7111, prediction=1.0),\n",
       " Row(id=7128, prediction=0.0),\n",
       " Row(id=7156, prediction=1.0),\n",
       " Row(id=7172, prediction=0.0),\n",
       " Row(id=7197, prediction=0.0),\n",
       " Row(id=7208, prediction=1.0),\n",
       " Row(id=7279, prediction=0.0),\n",
       " Row(id=7295, prediction=0.0),\n",
       " Row(id=7302, prediction=1.0),\n",
       " Row(id=7379, prediction=0.0),\n",
       " Row(id=7407, prediction=0.0),\n",
       " Row(id=7430, prediction=0.0),\n",
       " Row(id=7478, prediction=1.0),\n",
       " Row(id=7494, prediction=0.0),\n",
       " Row(id=7527, prediction=1.0),\n",
       " Row(id=7571, prediction=1.0),\n",
       " Row(id=7581, prediction=0.0),\n",
       " Row(id=7607, prediction=1.0),\n",
       " Row(id=7644, prediction=1.0),\n",
       " Row(id=7653, prediction=1.0),\n",
       " Row(id=7675, prediction=0.0),\n",
       " Row(id=7681, prediction=0.0),\n",
       " Row(id=7695, prediction=0.0),\n",
       " Row(id=7699, prediction=1.0),\n",
       " Row(id=7727, prediction=1.0),\n",
       " Row(id=7772, prediction=0.0),\n",
       " Row(id=7795, prediction=1.0),\n",
       " Row(id=7845, prediction=0.0),\n",
       " Row(id=7952, prediction=1.0),\n",
       " Row(id=7955, prediction=0.0),\n",
       " Row(id=7989, prediction=1.0),\n",
       " Row(id=8127, prediction=0.0),\n",
       " Row(id=8133, prediction=0.0),\n",
       " Row(id=8180, prediction=1.0),\n",
       " Row(id=8220, prediction=0.0),\n",
       " Row(id=8246, prediction=1.0),\n",
       " Row(id=8271, prediction=0.0),\n",
       " Row(id=8273, prediction=1.0),\n",
       " Row(id=8283, prediction=0.0),\n",
       " Row(id=8300, prediction=0.0),\n",
       " Row(id=8347, prediction=0.0),\n",
       " Row(id=8351, prediction=1.0),\n",
       " Row(id=8356, prediction=1.0),\n",
       " Row(id=8371, prediction=0.0),\n",
       " Row(id=8374, prediction=1.0),\n",
       " Row(id=8419, prediction=0.0),\n",
       " Row(id=8420, prediction=0.0),\n",
       " Row(id=8456, prediction=1.0),\n",
       " Row(id=8581, prediction=0.0),\n",
       " Row(id=8631, prediction=0.0),\n",
       " Row(id=8639, prediction=0.0),\n",
       " Row(id=8662, prediction=1.0),\n",
       " Row(id=8714, prediction=1.0),\n",
       " Row(id=8738, prediction=1.0),\n",
       " Row(id=8783, prediction=1.0),\n",
       " Row(id=8788, prediction=1.0),\n",
       " Row(id=8797, prediction=0.0),\n",
       " Row(id=8867, prediction=0.0),\n",
       " Row(id=8885, prediction=0.0),\n",
       " Row(id=8908, prediction=1.0),\n",
       " Row(id=8972, prediction=0.0),\n",
       " Row(id=8987, prediction=0.0),\n",
       " Row(id=8997, prediction=0.0),\n",
       " Row(id=9014, prediction=1.0),\n",
       " Row(id=9062, prediction=1.0),\n",
       " Row(id=9071, prediction=0.0),\n",
       " Row(id=9080, prediction=0.0),\n",
       " Row(id=9107, prediction=0.0),\n",
       " Row(id=9127, prediction=1.0),\n",
       " Row(id=9140, prediction=0.0),\n",
       " Row(id=9165, prediction=0.0),\n",
       " Row(id=9253, prediction=0.0),\n",
       " Row(id=9295, prediction=0.0),\n",
       " Row(id=9298, prediction=1.0),\n",
       " Row(id=9299, prediction=0.0),\n",
       " Row(id=9304, prediction=0.0),\n",
       " Row(id=9315, prediction=1.0),\n",
       " Row(id=9322, prediction=0.0),\n",
       " Row(id=9431, prediction=1.0),\n",
       " Row(id=9464, prediction=0.0),\n",
       " Row(id=9510, prediction=1.0),\n",
       " Row(id=9549, prediction=1.0),\n",
       " Row(id=9605, prediction=0.0),\n",
       " Row(id=9615, prediction=1.0),\n",
       " Row(id=9643, prediction=1.0),\n",
       " Row(id=9674, prediction=1.0),\n",
       " Row(id=9692, prediction=1.0),\n",
       " Row(id=9699, prediction=1.0),\n",
       " Row(id=9724, prediction=1.0),\n",
       " Row(id=9817, prediction=0.0),\n",
       " Row(id=9868, prediction=1.0),\n",
       " Row(id=9871, prediction=1.0),\n",
       " Row(id=9876, prediction=0.0),\n",
       " Row(id=9895, prediction=1.0),\n",
       " Row(id=9918, prediction=0.0),\n",
       " Row(id=9930, prediction=1.0),\n",
       " Row(id=9937, prediction=1.0),\n",
       " Row(id=10092, prediction=1.0),\n",
       " Row(id=10109, prediction=1.0),\n",
       " Row(id=10137, prediction=0.0),\n",
       " Row(id=10146, prediction=1.0),\n",
       " Row(id=10193, prediction=0.0),\n",
       " Row(id=10210, prediction=0.0),\n",
       " Row(id=10249, prediction=0.0),\n",
       " Row(id=10279, prediction=0.0),\n",
       " Row(id=10295, prediction=1.0),\n",
       " Row(id=10322, prediction=0.0),\n",
       " Row(id=10329, prediction=0.0),\n",
       " Row(id=10407, prediction=1.0),\n",
       " Row(id=10562, prediction=0.0),\n",
       " Row(id=10578, prediction=0.0),\n",
       " Row(id=10643, prediction=0.0),\n",
       " Row(id=10766, prediction=0.0),\n",
       " Row(id=10805, prediction=1.0),\n",
       " Row(id=10821, prediction=0.0),\n",
       " Row(id=10841, prediction=0.0),\n",
       " Row(id=10850, prediction=0.0),\n",
       " Row(id=10855, prediction=0.0),\n",
       " Row(id=10874, prediction=0.0),\n",
       " Row(id=10904, prediction=1.0),\n",
       " Row(id=11000, prediction=1.0),\n",
       " Row(id=11018, prediction=1.0),\n",
       " Row(id=11019, prediction=1.0),\n",
       " Row(id=11079, prediction=1.0),\n",
       " Row(id=11087, prediction=1.0),\n",
       " Row(id=11093, prediction=1.0),\n",
       " Row(id=11109, prediction=1.0),\n",
       " Row(id=11118, prediction=1.0),\n",
       " Row(id=11200, prediction=1.0),\n",
       " Row(id=11219, prediction=0.0),\n",
       " Row(id=11232, prediction=0.0),\n",
       " Row(id=11290, prediction=0.0),\n",
       " Row(id=11296, prediction=1.0),\n",
       " Row(id=11315, prediction=1.0),\n",
       " Row(id=11381, prediction=0.0),\n",
       " Row(id=11435, prediction=0.0),\n",
       " Row(id=11516, prediction=1.0),\n",
       " Row(id=11531, prediction=1.0),\n",
       " Row(id=11568, prediction=1.0),\n",
       " Row(id=11573, prediction=1.0),\n",
       " Row(id=11739, prediction=0.0),\n",
       " Row(id=11749, prediction=0.0),\n",
       " Row(id=11753, prediction=1.0),\n",
       " Row(id=11764, prediction=0.0),\n",
       " Row(id=11823, prediction=0.0),\n",
       " Row(id=11833, prediction=0.0),\n",
       " Row(id=11871, prediction=1.0),\n",
       " Row(id=12023, prediction=0.0),\n",
       " Row(id=12068, prediction=1.0),\n",
       " Row(id=12103, prediction=0.0),\n",
       " Row(id=12107, prediction=1.0),\n",
       " Row(id=12124, prediction=1.0),\n",
       " Row(id=12134, prediction=1.0),\n",
       " Row(id=12138, prediction=0.0),\n",
       " Row(id=12187, prediction=0.0),\n",
       " Row(id=12195, prediction=0.0),\n",
       " Row(id=12205, prediction=0.0),\n",
       " Row(id=12309, prediction=0.0),\n",
       " Row(id=12310, prediction=1.0),\n",
       " Row(id=12316, prediction=1.0),\n",
       " Row(id=12397, prediction=1.0),\n",
       " Row(id=12420, prediction=0.0),\n",
       " Row(id=12443, prediction=1.0),\n",
       " Row(id=12453, prediction=1.0),\n",
       " Row(id=12469, prediction=1.0),\n",
       " Row(id=12490, prediction=0.0),\n",
       " Row(id=12519, prediction=1.0),\n",
       " Row(id=12525, prediction=0.0),\n",
       " Row(id=12535, prediction=0.0),\n",
       " Row(id=12570, prediction=0.0),\n",
       " Row(id=12588, prediction=0.0),\n",
       " Row(id=12597, prediction=1.0),\n",
       " Row(id=12599, prediction=0.0),\n",
       " Row(id=12623, prediction=1.0),\n",
       " Row(id=12661, prediction=0.0),\n",
       " Row(id=12670, prediction=1.0),\n",
       " Row(id=12681, prediction=0.0),\n",
       " Row(id=12709, prediction=1.0),\n",
       " Row(id=12743, prediction=1.0),\n",
       " Row(id=12756, prediction=1.0),\n",
       " Row(id=12790, prediction=1.0),\n",
       " Row(id=12806, prediction=0.0),\n",
       " Row(id=12822, prediction=1.0),\n",
       " Row(id=12826, prediction=1.0),\n",
       " Row(id=12900, prediction=1.0),\n",
       " Row(id=12913, prediction=1.0),\n",
       " Row(id=12980, prediction=1.0),\n",
       " Row(id=13028, prediction=0.0),\n",
       " Row(id=13054, prediction=1.0),\n",
       " Row(id=13114, prediction=0.0),\n",
       " Row(id=13126, prediction=1.0),\n",
       " Row(id=13131, prediction=0.0),\n",
       " Row(id=13132, prediction=0.0),\n",
       " Row(id=13146, prediction=1.0),\n",
       " Row(id=13156, prediction=1.0),\n",
       " Row(id=13204, prediction=0.0),\n",
       " Row(id=13208, prediction=0.0),\n",
       " Row(id=13220, prediction=0.0),\n",
       " Row(id=13259, prediction=0.0),\n",
       " Row(id=13265, prediction=1.0),\n",
       " Row(id=13275, prediction=1.0),\n",
       " Row(id=13296, prediction=1.0),\n",
       " Row(id=13340, prediction=1.0),\n",
       " Row(id=13353, prediction=1.0),\n",
       " Row(id=13383, prediction=0.0),\n",
       " Row(id=13428, prediction=1.0),\n",
       " Row(id=13466, prediction=1.0),\n",
       " Row(id=13471, prediction=1.0),\n",
       " Row(id=13485, prediction=1.0),\n",
       " Row(id=13487, prediction=1.0),\n",
       " Row(id=13530, prediction=0.0),\n",
       " Row(id=13555, prediction=1.0),\n",
       " Row(id=13622, prediction=0.0),\n",
       " Row(id=13702, prediction=1.0),\n",
       " Row(id=13739, prediction=0.0),\n",
       " Row(id=13797, prediction=1.0),\n",
       " Row(id=13817, prediction=0.0),\n",
       " Row(id=13820, prediction=1.0),\n",
       " Row(id=13828, prediction=1.0),\n",
       " Row(id=13853, prediction=0.0),\n",
       " Row(id=13936, prediction=1.0),\n",
       " Row(id=13977, prediction=1.0),\n",
       " Row(id=13990, prediction=0.0),\n",
       " Row(id=14019, prediction=1.0),\n",
       " Row(id=14067, prediction=1.0),\n",
       " Row(id=14113, prediction=0.0),\n",
       " Row(id=14127, prediction=1.0),\n",
       " Row(id=14182, prediction=0.0),\n",
       " Row(id=14183, prediction=1.0),\n",
       " Row(id=14197, prediction=0.0),\n",
       " Row(id=14215, prediction=1.0),\n",
       " Row(id=14231, prediction=1.0),\n",
       " Row(id=14270, prediction=0.0),\n",
       " Row(id=14272, prediction=0.0),\n",
       " Row(id=14294, prediction=0.0),\n",
       " Row(id=14324, prediction=1.0),\n",
       " Row(id=14332, prediction=1.0),\n",
       " Row(id=14343, prediction=0.0),\n",
       " Row(id=14355, prediction=0.0),\n",
       " Row(id=14401, prediction=1.0),\n",
       " Row(id=14444, prediction=0.0),\n",
       " Row(id=14474, prediction=1.0),\n",
       " Row(id=14536, prediction=1.0),\n",
       " Row(id=14571, prediction=0.0),\n",
       " Row(id=14595, prediction=0.0),\n",
       " Row(id=14625, prediction=1.0),\n",
       " Row(id=14686, prediction=0.0),\n",
       " Row(id=14691, prediction=0.0),\n",
       " Row(id=14705, prediction=1.0),\n",
       " Row(id=14739, prediction=0.0),\n",
       " Row(id=14746, prediction=1.0),\n",
       " Row(id=14752, prediction=0.0),\n",
       " Row(id=14781, prediction=0.0),\n",
       " Row(id=14792, prediction=1.0),\n",
       " Row(id=14803, prediction=1.0),\n",
       " Row(id=14820, prediction=1.0),\n",
       " Row(id=14830, prediction=0.0),\n",
       " Row(id=14853, prediction=1.0),\n",
       " Row(id=14856, prediction=1.0),\n",
       " Row(id=14951, prediction=0.0),\n",
       " Row(id=15008, prediction=0.0),\n",
       " Row(id=15033, prediction=1.0),\n",
       " Row(id=15067, prediction=1.0),\n",
       " Row(id=15160, prediction=1.0),\n",
       " Row(id=15182, prediction=1.0),\n",
       " Row(id=15201, prediction=0.0),\n",
       " Row(id=15231, prediction=1.0),\n",
       " Row(id=15261, prediction=1.0),\n",
       " Row(id=15266, prediction=1.0),\n",
       " Row(id=15368, prediction=0.0),\n",
       " Row(id=15371, prediction=0.0),\n",
       " Row(id=15391, prediction=0.0),\n",
       " Row(id=15467, prediction=1.0),\n",
       " Row(id=15511, prediction=1.0),\n",
       " Row(id=15518, prediction=1.0),\n",
       " Row(id=15556, prediction=0.0),\n",
       " Row(id=15564, prediction=1.0),\n",
       " Row(id=15572, prediction=1.0),\n",
       " Row(id=15603, prediction=0.0),\n",
       " Row(id=15635, prediction=1.0),\n",
       " Row(id=15738, prediction=1.0),\n",
       " Row(id=15791, prediction=0.0),\n",
       " Row(id=15840, prediction=1.0),\n",
       " Row(id=15841, prediction=1.0),\n",
       " Row(id=15872, prediction=0.0),\n",
       " Row(id=15879, prediction=0.0),\n",
       " Row(id=15934, prediction=0.0),\n",
       " Row(id=15940, prediction=0.0),\n",
       " Row(id=15995, prediction=0.0),\n",
       " Row(id=16008, prediction=0.0),\n",
       " Row(id=16022, prediction=1.0),\n",
       " Row(id=16037, prediction=1.0),\n",
       " Row(id=16046, prediction=0.0),\n",
       " Row(id=16051, prediction=0.0),\n",
       " Row(id=16054, prediction=0.0),\n",
       " Row(id=16062, prediction=0.0),\n",
       " Row(id=16089, prediction=1.0),\n",
       " Row(id=16106, prediction=0.0),\n",
       " Row(id=16108, prediction=0.0),\n",
       " Row(id=16160, prediction=0.0),\n",
       " Row(id=16174, prediction=0.0),\n",
       " Row(id=16194, prediction=0.0),\n",
       " Row(id=16198, prediction=1.0),\n",
       " Row(id=16233, prediction=0.0),\n",
       " Row(id=16304, prediction=1.0),\n",
       " Row(id=16305, prediction=0.0),\n",
       " Row(id=16308, prediction=1.0),\n",
       " Row(id=16398, prediction=1.0),\n",
       " Row(id=16416, prediction=1.0),\n",
       " Row(id=16509, prediction=1.0),\n",
       " Row(id=16516, prediction=0.0),\n",
       " Row(id=16546, prediction=1.0),\n",
       " Row(id=16576, prediction=0.0),\n",
       " Row(id=16641, prediction=1.0),\n",
       " Row(id=16680, prediction=0.0),\n",
       " Row(id=16688, prediction=0.0),\n",
       " Row(id=16770, prediction=1.0),\n",
       " Row(id=16779, prediction=1.0),\n",
       " Row(id=16803, prediction=1.0),\n",
       " Row(id=16847, prediction=0.0),\n",
       " Row(id=16857, prediction=0.0),\n",
       " Row(id=16874, prediction=1.0),\n",
       " Row(id=16882, prediction=1.0),\n",
       " Row(id=16894, prediction=0.0),\n",
       " Row(id=16938, prediction=0.0),\n",
       " Row(id=16974, prediction=1.0),\n",
       " Row(id=17028, prediction=0.0),\n",
       " Row(id=17068, prediction=0.0),\n",
       " Row(id=17091, prediction=0.0),\n",
       " Row(id=17110, prediction=1.0),\n",
       " Row(id=17112, prediction=1.0),\n",
       " Row(id=17137, prediction=0.0),\n",
       " Row(id=17141, prediction=0.0),\n",
       " Row(id=17156, prediction=1.0),\n",
       " Row(id=17173, prediction=1.0),\n",
       " Row(id=17175, prediction=1.0),\n",
       " Row(id=17176, prediction=1.0),\n",
       " Row(id=17229, prediction=1.0),\n",
       " Row(id=17280, prediction=1.0),\n",
       " Row(id=17378, prediction=0.0),\n",
       " Row(id=17391, prediction=0.0),\n",
       " Row(id=17413, prediction=0.0),\n",
       " Row(id=17417, prediction=1.0),\n",
       " Row(id=17435, prediction=1.0),\n",
       " Row(id=17462, prediction=0.0),\n",
       " Row(id=17549, prediction=0.0),\n",
       " Row(id=17559, prediction=0.0),\n",
       " Row(id=17617, prediction=1.0),\n",
       " Row(id=17683, prediction=0.0),\n",
       " Row(id=17706, prediction=0.0),\n",
       " Row(id=17720, prediction=1.0),\n",
       " Row(id=17724, prediction=0.0),\n",
       " Row(id=17745, prediction=1.0),\n",
       " Row(id=17782, prediction=0.0),\n",
       " Row(id=17788, prediction=1.0),\n",
       " Row(id=17846, prediction=0.0),\n",
       " Row(id=17853, prediction=1.0),\n",
       " Row(id=17876, prediction=0.0),\n",
       " Row(id=17899, prediction=0.0),\n",
       " Row(id=17918, prediction=0.0),\n",
       " Row(id=17964, prediction=0.0),\n",
       " Row(id=17967, prediction=1.0),\n",
       " Row(id=17987, prediction=1.0),\n",
       " Row(id=18004, prediction=1.0),\n",
       " Row(id=18030, prediction=0.0),\n",
       " Row(id=18056, prediction=0.0),\n",
       " Row(id=18076, prediction=1.0),\n",
       " Row(id=18105, prediction=1.0),\n",
       " Row(id=18123, prediction=1.0),\n",
       " Row(id=18205, prediction=0.0),\n",
       " Row(id=18263, prediction=1.0),\n",
       " Row(id=18274, prediction=0.0),\n",
       " Row(id=18279, prediction=1.0),\n",
       " Row(id=18299, prediction=1.0),\n",
       " Row(id=18313, prediction=0.0),\n",
       " Row(id=18339, prediction=1.0),\n",
       " Row(id=18353, prediction=1.0),\n",
       " Row(id=18359, prediction=0.0),\n",
       " Row(id=18375, prediction=1.0),\n",
       " Row(id=18475, prediction=1.0),\n",
       " Row(id=18477, prediction=0.0),\n",
       " Row(id=18478, prediction=0.0),\n",
       " Row(id=18490, prediction=1.0),\n",
       " Row(id=18511, prediction=0.0),\n",
       " Row(id=18519, prediction=1.0),\n",
       " Row(id=18533, prediction=0.0),\n",
       " Row(id=18542, prediction=0.0),\n",
       " Row(id=18579, prediction=1.0),\n",
       " Row(id=18587, prediction=0.0),\n",
       " Row(id=18595, prediction=1.0),\n",
       " Row(id=18596, prediction=1.0),\n",
       " Row(id=18616, prediction=1.0),\n",
       " Row(id=18694, prediction=1.0),\n",
       " Row(id=18707, prediction=1.0),\n",
       " Row(id=18709, prediction=1.0),\n",
       " Row(id=18770, prediction=1.0),\n",
       " Row(id=18836, prediction=1.0),\n",
       " Row(id=18849, prediction=1.0),\n",
       " Row(id=18876, prediction=1.0),\n",
       " Row(id=18913, prediction=0.0),\n",
       " Row(id=18917, prediction=0.0),\n",
       " Row(id=18941, prediction=0.0),\n",
       " Row(id=18950, prediction=0.0),\n",
       " Row(id=18969, prediction=1.0),\n",
       " Row(id=18973, prediction=0.0),\n",
       " Row(id=18976, prediction=1.0),\n",
       " Row(id=19015, prediction=1.0),\n",
       " Row(id=19024, prediction=0.0),\n",
       " Row(id=19082, prediction=1.0),\n",
       " Row(id=19143, prediction=1.0),\n",
       " Row(id=19158, prediction=1.0),\n",
       " Row(id=19169, prediction=1.0),\n",
       " Row(id=19189, prediction=0.0),\n",
       " Row(id=19200, prediction=1.0),\n",
       " Row(id=19226, prediction=1.0),\n",
       " Row(id=19342, prediction=1.0),\n",
       " Row(id=19385, prediction=1.0),\n",
       " Row(id=19445, prediction=1.0),\n",
       " Row(id=19448, prediction=0.0),\n",
       " Row(id=19457, prediction=0.0),\n",
       " Row(id=19471, prediction=0.0),\n",
       " Row(id=19479, prediction=1.0),\n",
       " Row(id=19487, prediction=0.0),\n",
       " Row(id=19571, prediction=1.0),\n",
       " Row(id=19583, prediction=0.0),\n",
       " Row(id=19590, prediction=1.0),\n",
       " Row(id=19602, prediction=1.0),\n",
       " Row(id=19661, prediction=0.0),\n",
       " Row(id=19696, prediction=1.0),\n",
       " Row(id=19761, prediction=0.0),\n",
       " Row(id=19915, prediction=1.0),\n",
       " Row(id=20026, prediction=1.0),\n",
       " Row(id=20032, prediction=1.0),\n",
       " Row(id=20065, prediction=0.0),\n",
       " Row(id=20093, prediction=0.0),\n",
       " Row(id=20124, prediction=1.0),\n",
       " Row(id=20125, prediction=1.0),\n",
       " Row(id=20136, prediction=1.0),\n",
       " Row(id=20158, prediction=0.0),\n",
       " Row(id=20166, prediction=1.0),\n",
       " Row(id=20187, prediction=1.0),\n",
       " Row(id=20243, prediction=0.0),\n",
       " Row(id=20248, prediction=0.0),\n",
       " Row(id=20271, prediction=1.0),\n",
       " Row(id=20324, prediction=1.0),\n",
       " Row(id=20357, prediction=1.0),\n",
       " Row(id=20402, prediction=1.0),\n",
       " Row(id=20414, prediction=1.0),\n",
       " Row(id=20441, prediction=0.0),\n",
       " Row(id=20533, prediction=0.0),\n",
       " Row(id=20545, prediction=1.0),\n",
       " Row(id=20563, prediction=0.0),\n",
       " Row(id=20568, prediction=1.0),\n",
       " Row(id=20608, prediction=0.0),\n",
       " Row(id=20623, prediction=0.0),\n",
       " Row(id=20639, prediction=1.0),\n",
       " Row(id=20645, prediction=1.0),\n",
       " Row(id=20655, prediction=1.0),\n",
       " Row(id=20660, prediction=1.0),\n",
       " Row(id=20723, prediction=0.0),\n",
       " Row(id=20773, prediction=1.0),\n",
       " Row(id=20777, prediction=1.0),\n",
       " Row(id=20790, prediction=0.0),\n",
       " Row(id=20834, prediction=1.0),\n",
       " Row(id=20862, prediction=1.0),\n",
       " Row(id=20946, prediction=0.0),\n",
       " Row(id=20948, prediction=1.0),\n",
       " Row(id=20973, prediction=0.0),\n",
       " Row(id=21036, prediction=1.0),\n",
       " Row(id=21092, prediction=1.0),\n",
       " Row(id=21115, prediction=0.0),\n",
       " Row(id=21121, prediction=0.0),\n",
       " Row(id=21218, prediction=0.0),\n",
       " Row(id=21283, prediction=0.0),\n",
       " Row(id=21303, prediction=1.0),\n",
       " Row(id=21343, prediction=1.0),\n",
       " Row(id=21355, prediction=1.0),\n",
       " Row(id=21392, prediction=1.0),\n",
       " Row(id=21415, prediction=0.0),\n",
       " Row(id=21512, prediction=0.0),\n",
       " Row(id=21554, prediction=1.0),\n",
       " Row(id=21565, prediction=1.0),\n",
       " Row(id=21568, prediction=1.0),\n",
       " Row(id=21623, prediction=1.0),\n",
       " Row(id=21628, prediction=0.0),\n",
       " Row(id=21665, prediction=1.0),\n",
       " Row(id=21724, prediction=0.0),\n",
       " Row(id=21742, prediction=0.0),\n",
       " Row(id=21752, prediction=1.0),\n",
       " Row(id=21847, prediction=1.0),\n",
       " Row(id=21854, prediction=0.0),\n",
       " Row(id=21933, prediction=0.0),\n",
       " Row(id=21946, prediction=0.0),\n",
       " Row(id=22037, prediction=1.0),\n",
       " Row(id=22040, prediction=0.0),\n",
       " Row(id=22045, prediction=1.0),\n",
       " Row(id=22065, prediction=1.0),\n",
       " Row(id=22191, prediction=0.0),\n",
       " Row(id=22193, prediction=1.0),\n",
       " Row(id=22234, prediction=1.0),\n",
       " Row(id=22252, prediction=0.0),\n",
       " Row(id=22255, prediction=0.0),\n",
       " Row(id=22284, prediction=1.0),\n",
       " Row(id=22294, prediction=0.0),\n",
       " Row(id=22345, prediction=1.0),\n",
       " Row(id=22346, prediction=1.0),\n",
       " Row(id=22388, prediction=0.0),\n",
       " Row(id=22444, prediction=1.0),\n",
       " Row(id=22508, prediction=0.0),\n",
       " Row(id=22539, prediction=0.0),\n",
       " Row(id=22542, prediction=1.0),\n",
       " Row(id=22555, prediction=0.0),\n",
       " Row(id=22627, prediction=0.0),\n",
       " Row(id=22653, prediction=1.0),\n",
       " Row(id=22716, prediction=1.0),\n",
       " Row(id=22742, prediction=1.0),\n",
       " Row(id=22765, prediction=1.0),\n",
       " Row(id=22796, prediction=0.0),\n",
       " Row(id=22843, prediction=0.0),\n",
       " Row(id=22861, prediction=1.0),\n",
       " Row(id=22920, prediction=0.0),\n",
       " Row(id=22924, prediction=1.0),\n",
       " Row(id=23079, prediction=1.0),\n",
       " Row(id=23154, prediction=0.0),\n",
       " Row(id=23173, prediction=1.0),\n",
       " Row(id=23210, prediction=1.0),\n",
       " Row(id=23214, prediction=1.0),\n",
       " Row(id=23232, prediction=1.0),\n",
       " Row(id=23303, prediction=0.0),\n",
       " Row(id=23382, prediction=0.0),\n",
       " Row(id=23415, prediction=0.0),\n",
       " Row(id=23417, prediction=0.0),\n",
       " Row(id=23429, prediction=1.0),\n",
       " Row(id=23431, prediction=0.0),\n",
       " Row(id=23439, prediction=0.0),\n",
       " Row(id=23463, prediction=1.0),\n",
       " Row(id=23484, prediction=0.0),\n",
       " Row(id=23493, prediction=0.0),\n",
       " Row(id=23548, prediction=1.0),\n",
       " Row(id=23556, prediction=1.0),\n",
       " Row(id=23607, prediction=0.0),\n",
       " Row(id=23647, prediction=0.0),\n",
       " Row(id=23666, prediction=1.0),\n",
       " Row(id=23703, prediction=0.0),\n",
       " Row(id=23713, prediction=1.0),\n",
       " Row(id=23773, prediction=0.0),\n",
       " Row(id=23778, prediction=0.0),\n",
       " Row(id=23811, prediction=0.0),\n",
       " Row(id=23826, prediction=0.0),\n",
       " Row(id=23838, prediction=0.0),\n",
       " Row(id=23860, prediction=1.0),\n",
       " Row(id=23914, prediction=0.0),\n",
       " Row(id=23927, prediction=0.0),\n",
       " Row(id=23954, prediction=1.0),\n",
       " Row(id=23985, prediction=1.0),\n",
       " Row(id=24021, prediction=1.0),\n",
       " Row(id=24061, prediction=1.0),\n",
       " Row(id=24074, prediction=0.0),\n",
       " Row(id=24115, prediction=0.0),\n",
       " Row(id=24164, prediction=1.0),\n",
       " Row(id=24181, prediction=1.0),\n",
       " Row(id=24244, prediction=1.0),\n",
       " Row(id=24246, prediction=0.0),\n",
       " Row(id=24281, prediction=0.0),\n",
       " Row(id=24343, prediction=1.0),\n",
       " Row(id=24346, prediction=0.0),\n",
       " Row(id=24365, prediction=1.0),\n",
       " Row(id=24373, prediction=0.0),\n",
       " Row(id=24514, prediction=0.0),\n",
       " Row(id=24527, prediction=0.0),\n",
       " Row(id=24530, prediction=0.0),\n",
       " Row(id=24531, prediction=1.0),\n",
       " Row(id=24572, prediction=1.0),\n",
       " Row(id=24593, prediction=0.0),\n",
       " Row(id=24595, prediction=0.0),\n",
       " Row(id=24600, prediction=1.0),\n",
       " Row(id=24604, prediction=0.0),\n",
       " Row(id=24641, prediction=0.0),\n",
       " Row(id=24662, prediction=0.0),\n",
       " Row(id=24701, prediction=1.0),\n",
       " Row(id=24729, prediction=1.0),\n",
       " Row(id=24804, prediction=1.0),\n",
       " Row(id=24840, prediction=1.0),\n",
       " Row(id=24878, prediction=1.0),\n",
       " Row(id=24904, prediction=1.0),\n",
       " Row(id=24910, prediction=1.0),\n",
       " Row(id=24980, prediction=1.0),\n",
       " Row(id=24992, prediction=0.0),\n",
       " Row(id=25037, prediction=1.0),\n",
       " Row(id=25051, prediction=0.0),\n",
       " Row(id=25069, prediction=0.0),\n",
       " Row(id=25080, prediction=0.0),\n",
       " Row(id=25083, prediction=1.0),\n",
       " Row(id=25148, prediction=1.0),\n",
       " Row(id=25151, prediction=1.0),\n",
       " Row(id=25179, prediction=1.0),\n",
       " Row(id=25190, prediction=0.0),\n",
       " Row(id=25200, prediction=0.0),\n",
       " Row(id=25204, prediction=0.0),\n",
       " Row(id=25286, prediction=0.0),\n",
       " Row(id=25289, prediction=1.0),\n",
       " Row(id=25290, prediction=1.0),\n",
       " Row(id=25334, prediction=1.0),\n",
       " Row(id=25335, prediction=1.0),\n",
       " Row(id=25353, prediction=0.0),\n",
       " Row(id=25440, prediction=1.0),\n",
       " Row(id=25445, prediction=1.0),\n",
       " Row(id=25451, prediction=1.0),\n",
       " Row(id=25468, prediction=1.0),\n",
       " Row(id=25490, prediction=1.0),\n",
       " Row(id=25506, prediction=0.0),\n",
       " Row(id=25513, prediction=1.0),\n",
       " Row(id=25542, prediction=0.0),\n",
       " Row(id=25575, prediction=1.0),\n",
       " Row(id=25577, prediction=0.0),\n",
       " Row(id=25615, prediction=0.0),\n",
       " Row(id=25641, prediction=1.0),\n",
       " Row(id=25704, prediction=1.0),\n",
       " Row(id=25735, prediction=1.0),\n",
       " Row(id=25745, prediction=1.0),\n",
       " Row(id=25780, prediction=1.0),\n",
       " Row(id=25786, prediction=0.0),\n",
       " Row(id=25811, prediction=1.0),\n",
       " Row(id=25851, prediction=0.0),\n",
       " Row(id=25875, prediction=1.0),\n",
       " Row(id=25940, prediction=0.0),\n",
       " Row(id=25986, prediction=1.0),\n",
       " Row(id=25992, prediction=0.0),\n",
       " Row(id=26001, prediction=0.0),\n",
       " Row(id=26005, prediction=0.0),\n",
       " Row(id=26044, prediction=1.0),\n",
       " Row(id=26082, prediction=0.0),\n",
       " Row(id=26144, prediction=0.0),\n",
       " Row(id=26164, prediction=0.0),\n",
       " Row(id=26170, prediction=0.0),\n",
       " Row(id=26197, prediction=1.0),\n",
       " Row(id=26244, prediction=1.0),\n",
       " Row(id=26259, prediction=1.0),\n",
       " Row(id=26276, prediction=0.0),\n",
       " Row(id=26288, prediction=1.0),\n",
       " Row(id=26300, prediction=1.0),\n",
       " Row(id=26329, prediction=1.0),\n",
       " Row(id=26380, prediction=0.0),\n",
       " Row(id=26381, prediction=1.0),\n",
       " Row(id=26467, prediction=0.0),\n",
       " Row(id=26493, prediction=1.0),\n",
       " Row(id=26567, prediction=0.0),\n",
       " Row(id=26591, prediction=0.0),\n",
       " Row(id=26622, prediction=0.0),\n",
       " Row(id=26623, prediction=0.0),\n",
       " Row(id=26637, prediction=0.0),\n",
       " Row(id=26648, prediction=1.0),\n",
       " Row(id=26661, prediction=0.0),\n",
       " Row(id=26699, prediction=0.0),\n",
       " Row(id=26701, prediction=0.0),\n",
       " Row(id=26712, prediction=1.0),\n",
       " Row(id=26748, prediction=1.0),\n",
       " Row(id=26757, prediction=1.0),\n",
       " Row(id=26771, prediction=0.0),\n",
       " Row(id=26817, prediction=0.0),\n",
       " Row(id=26837, prediction=0.0),\n",
       " Row(id=26886, prediction=0.0),\n",
       " Row(id=26916, prediction=0.0),\n",
       " Row(id=26974, prediction=0.0),\n",
       " Row(id=27000, prediction=0.0),\n",
       " Row(id=27007, prediction=0.0),\n",
       " Row(id=27025, prediction=0.0),\n",
       " Row(id=27027, prediction=1.0),\n",
       " Row(id=27030, prediction=0.0),\n",
       " Row(id=27097, prediction=1.0),\n",
       " Row(id=27116, prediction=0.0),\n",
       " Row(id=27120, prediction=1.0),\n",
       " Row(id=27138, prediction=1.0),\n",
       " Row(id=27167, prediction=0.0),\n",
       " Row(id=27175, prediction=0.0),\n",
       " Row(id=27202, prediction=1.0),\n",
       " Row(id=27208, prediction=0.0),\n",
       " Row(id=27210, prediction=1.0),\n",
       " Row(id=27228, prediction=1.0),\n",
       " Row(id=27276, prediction=0.0),\n",
       " Row(id=27292, prediction=1.0),\n",
       " Row(id=27294, prediction=1.0),\n",
       " Row(id=27356, prediction=1.0),\n",
       " Row(id=27363, prediction=0.0),\n",
       " Row(id=27378, prediction=0.0),\n",
       " Row(id=27426, prediction=1.0),\n",
       " Row(id=27443, prediction=1.0),\n",
       " Row(id=27508, prediction=1.0),\n",
       " Row(id=27540, prediction=1.0),\n",
       " Row(id=27560, prediction=0.0),\n",
       " Row(id=27607, prediction=1.0),\n",
       " Row(id=27624, prediction=0.0),\n",
       " Row(id=27626, prediction=1.0),\n",
       " Row(id=27670, prediction=0.0),\n",
       " Row(id=27691, prediction=1.0),\n",
       " Row(id=27700, prediction=1.0),\n",
       " Row(id=27724, prediction=1.0),\n",
       " Row(id=27766, prediction=1.0),\n",
       " Row(id=27770, prediction=1.0),\n",
       " Row(id=27786, prediction=0.0),\n",
       " Row(id=27895, prediction=0.0),\n",
       " Row(id=27946, prediction=1.0),\n",
       " Row(id=28040, prediction=1.0),\n",
       " Row(id=28068, prediction=0.0),\n",
       " Row(id=28077, prediction=0.0),\n",
       " Row(id=28109, prediction=1.0),\n",
       " Row(id=28165, prediction=0.0),\n",
       " Row(id=28177, prediction=0.0),\n",
       " Row(id=28245, prediction=1.0),\n",
       " Row(id=28395, prediction=1.0),\n",
       " Row(id=28412, prediction=0.0),\n",
       " Row(id=28437, prediction=0.0),\n",
       " Row(id=28461, prediction=0.0),\n",
       " Row(id=28590, prediction=1.0),\n",
       " Row(id=28597, prediction=0.0),\n",
       " Row(id=28620, prediction=0.0),\n",
       " Row(id=28623, prediction=0.0),\n",
       " Row(id=28633, prediction=1.0),\n",
       " Row(id=28665, prediction=0.0),\n",
       " Row(id=28769, prediction=1.0),\n",
       " Row(id=28845, prediction=0.0),\n",
       " Row(id=28848, prediction=1.0),\n",
       " Row(id=28852, prediction=1.0),\n",
       " Row(id=28872, prediction=1.0),\n",
       " Row(id=28879, prediction=1.0),\n",
       " Row(id=28918, prediction=1.0),\n",
       " Row(id=28939, prediction=0.0),\n",
       " Row(id=28951, prediction=0.0),\n",
       " Row(id=29013, prediction=1.0),\n",
       " Row(id=29033, prediction=1.0),\n",
       " Row(id=29044, prediction=1.0),\n",
       " Row(id=29052, prediction=1.0),\n",
       " Row(id=29059, prediction=0.0),\n",
       " Row(id=29066, prediction=0.0),\n",
       " Row(id=29104, prediction=0.0),\n",
       " Row(id=29116, prediction=0.0),\n",
       " Row(id=29121, prediction=1.0),\n",
       " Row(id=29187, prediction=1.0),\n",
       " Row(id=29188, prediction=0.0),\n",
       " Row(id=29201, prediction=1.0),\n",
       " Row(id=29222, prediction=0.0),\n",
       " Row(id=29268, prediction=1.0),\n",
       " Row(id=29271, prediction=0.0),\n",
       " Row(id=29286, prediction=0.0),\n",
       " Row(id=29327, prediction=1.0),\n",
       " Row(id=29349, prediction=1.0),\n",
       " Row(id=29363, prediction=1.0),\n",
       " Row(id=29367, prediction=0.0),\n",
       " Row(id=29398, prediction=0.0),\n",
       " Row(id=29408, prediction=0.0),\n",
       " Row(id=29422, prediction=0.0),\n",
       " Row(id=29544, prediction=0.0),\n",
       " Row(id=29557, prediction=0.0),\n",
       " Row(id=29653, prediction=1.0),\n",
       " Row(id=29661, prediction=0.0),\n",
       " Row(id=29669, prediction=1.0),\n",
       " Row(id=29691, prediction=0.0),\n",
       " Row(id=29702, prediction=1.0),\n",
       " Row(id=29703, prediction=0.0),\n",
       " Row(id=29712, prediction=0.0),\n",
       " Row(id=29731, prediction=1.0),\n",
       " Row(id=29746, prediction=0.0),\n",
       " Row(id=29801, prediction=1.0),\n",
       " Row(id=29816, prediction=1.0),\n",
       " Row(id=29862, prediction=0.0),\n",
       " Row(id=29890, prediction=1.0),\n",
       " Row(id=29981, prediction=1.0),\n",
       " Row(id=29984, prediction=1.0),\n",
       " Row(id=29987, prediction=0.0),\n",
       " Row(id=30045, prediction=1.0),\n",
       " Row(id=30051, prediction=0.0),\n",
       " Row(id=30075, prediction=1.0),\n",
       " Row(id=30110, prediction=0.0),\n",
       " Row(id=30171, prediction=1.0),\n",
       " Row(id=30282, prediction=0.0),\n",
       " Row(id=30300, prediction=0.0),\n",
       " Row(id=30313, prediction=1.0),\n",
       " Row(id=30329, prediction=0.0),\n",
       " Row(id=30394, prediction=0.0),\n",
       " Row(id=30447, prediction=0.0),\n",
       " Row(id=30478, prediction=0.0),\n",
       " Row(id=30490, prediction=1.0),\n",
       " Row(id=30521, prediction=0.0),\n",
       " Row(id=30527, prediction=1.0),\n",
       " Row(id=30620, prediction=0.0),\n",
       " Row(id=30640, prediction=1.0),\n",
       " Row(id=30653, prediction=1.0),\n",
       " Row(id=30673, prediction=1.0),\n",
       " Row(id=30680, prediction=1.0),\n",
       " Row(id=30778, prediction=1.0),\n",
       " Row(id=30781, prediction=0.0),\n",
       " Row(id=30788, prediction=1.0),\n",
       " Row(id=30803, prediction=1.0),\n",
       " Row(id=30813, prediction=0.0),\n",
       " Row(id=30835, prediction=0.0),\n",
       " Row(id=30869, prediction=0.0),\n",
       " Row(id=30930, prediction=1.0),\n",
       " ...]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_selected = selected.sort(\"id\").collect()\n",
    "sort_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4649"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [int(x['prediction']) for x in sort_selected]\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
